{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 06 - Modeling: Battle Outcome Prediction\n",
    "\n",
    "**Purpose**: Build predictive models for technical rigor scoring.\n",
    "\n",
    "**Goal**: Predict battle outcomes based on deck composition alone.\n",
    "\n",
    "**Models to Try**:\n",
    "1. Logistic Regression (baseline)\n",
    "2. Random Forest (feature importance insights)\n",
    "3. XGBoost (likely best performance)\n",
    "\n",
    "**Key Metrics**:\n",
    "- Accuracy\n",
    "- Precision/Recall\n",
    "- ROC-AUC\n",
    "- Feature importance (for insights!)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "âœ“ Presentation style configured\n"
     ]
    }
   ],
   "source": [
    "import sys, os, pandas as pd, numpy as np\n",
    "import matplotlib.pyplot as plt, seaborn as sns\n",
    "\n",
    "from sklearn.model_selection import train_test_split, cross_val_score\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.metrics import accuracy_score, classification_report, roc_auc_score, confusion_matrix\n",
    "import xgboost as xgb\n",
    "\n",
    "PROJECT_ROOT = os.path.abspath(os.path.join(os.getcwd(), '..'))\n",
    "sys.path.insert(0, os.path.join(PROJECT_ROOT, 'src'))\n",
    "\n",
    "from visualization import setup_presentation_style\n",
    "setup_presentation_style()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Load Feature Matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "# Try to load engineered features from notebook 05\n# If not available, create a basic feature set\ntry:\n    features = pd.read_parquet(os.path.join(PROJECT_ROOT, 'artifacts/model_features.parquet'))\n    print(f\"âœ“ Loaded {len(features):,} battles with {len(features.columns)} features\")\nexcept FileNotFoundError:\n    print(\"âš  Feature matrix not found. Creating basic feature set from raw data...\")\n    # Create a sample directly from battles view\n    import sys\n    sys.path.insert(0, os.path.join(PROJECT_ROOT, 'src'))\n    from duckdb_utils import get_connection, create_battles_view, query_to_df\n    \n    con = get_connection()\n    create_battles_view(con, os.path.join(PROJECT_ROOT, 'battles.parquet'))\n    \n    sample_query = \"\"\"\n    SELECT * FROM battles\n    USING SAMPLE 10% (bernoulli)\n    \"\"\"\n    features = query_to_df(con, sample_query, show_progress=False)\n    print(f\"âœ“ Created sample with {len(features):,} battles\")\n\nprint(f\"\\nDataset shape: {features.shape}\")\nprint(f\"Columns: {list(features.columns[:10])}...\")  # Show first 10 columns"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Prepare Data for Modeling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "# Restructure data: each battle becomes 2 rows (one for each player)\n# Row 1: winner's perspective (outcome=1)\n# Row 2: loser's perspective (outcome=0)\n\nprint(\"Restructuring data for binary classification...\")\n\n# Select features to use for modeling\nfeature_cols = []\n\n# Add numerical features\nnumerical_features = [\n    'winner.elixir.average', 'winner.troop.count', 'winner.spell.count', \n    'winner.structure.count', 'winner.rarity.legendary.count',\n    'winner.rarity.epic.count', 'winner.rarity.rare.count', 'winner.rarity.common.count',\n    'winner.startingTrophies'\n]\n\n# Check which columns exist\nfor col in numerical_features:\n    if col in features.columns:\n        feature_cols.append(col)\n\nprint(f\"Using {len(feature_cols)} features for modeling\")\n\n# Create winner rows (outcome=1)\nwinner_data = features[feature_cols].copy()\nwinner_data['outcome'] = 1\n\n# Create loser rows (outcome=0) by replacing 'winner' with 'loser' in column names\nloser_feature_cols = [col.replace('winner', 'loser') for col in feature_cols]\nloser_data = features[loser_feature_cols].copy()\nloser_data.columns = feature_cols  # Rename to match winner columns\nloser_data['outcome'] = 0\n\n# Combine winner and loser rows\nmodeling_data = pd.concat([winner_data, loser_data], ignore_index=True)\n\n# Drop rows with missing values\nmodeling_data = modeling_data.dropna()\n\nprint(f\"\\nâœ“ Restructured dataset:\")\nprint(f\"  - Original battles: {len(features):,}\")\nprint(f\"  - Total rows (2x battles): {len(modeling_data):,}\")\nprint(f\"  - Features: {len(feature_cols)}\")\nprint(f\"  - Outcome distribution:\")\nprint(f\"    â€¢ Wins: {(modeling_data['outcome'] == 1).sum():,} ({(modeling_data['outcome'] == 1).mean()*100:.1f}%)\")\nprint(f\"    â€¢ Losses: {(modeling_data['outcome'] == 0).sum():,} ({(modeling_data['outcome'] == 0).mean()*100:.1f}%)\")\n\n# Define X and y\ny = modeling_data['outcome']\nX = modeling_data.drop('outcome', axis=1)\n\nprint(f\"\\nâœ“ Ready for model training\")\nprint(f\"  X shape: {X.shape}\")\nprint(f\"  y shape: {y.shape}\")"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Train/Test Split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "# Create train/test split (80/20)\nX_train, X_test, y_train, y_test = train_test_split(\n    X, y, test_size=0.2, random_state=42, stratify=y\n)\n\nprint(f\"âœ“ Train/Test Split Complete:\")\nprint(f\"  Training set: {len(X_train):,} samples ({len(X_train) / len(X) * 100:.1f}%)\")\nprint(f\"  Test set: {len(X_test):,} samples ({len(X_test) / len(X) * 100:.1f}%)\")\nprint(f\"\\n  Training outcome distribution:\")\nprint(f\"    â€¢ Wins: {(y_train == 1).sum():,} ({(y_train == 1).mean() * 100:.1f}%)\")\nprint(f\"    â€¢ Losses: {(y_train == 0).sum():,} ({(y_train == 0).mean() * 100:.1f}%)\")\nprint(f\"\\n  Test outcome distribution:\")\nprint(f\"    â€¢ Wins: {(y_test == 1).sum():,} ({(y_test == 1).mean() * 100:.1f}%)\")\nprint(f\"    â€¢ Losses: {(y_test == 0).sum():,} ({(y_test == 0).mean() * 100:.1f}%)\")"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Model 1: Logistic Regression (Baseline)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "# Train Logistic Regression (baseline model)\nprint(\"Training Logistic Regression...\")\nlr_model = LogisticRegression(max_iter=1000, random_state=42, n_jobs=-1)\nlr_model.fit(X_train, y_train)\n\n# Predictions\nlr_pred = lr_model.predict(X_test)\nlr_pred_proba = lr_model.predict_proba(X_test)[:, 1]\n\n# Evaluation metrics\nlr_acc = accuracy_score(y_test, lr_pred)\nlr_roc_auc = roc_auc_score(y_test, lr_pred_proba)\n\nprint(f\"\\nâœ“ Logistic Regression Results:\")\nprint(f\"  Accuracy: {lr_acc:.4f} ({lr_acc*100:.2f}%)\")\nprint(f\"  ROC-AUC: {lr_roc_auc:.4f}\")\nprint(f\"\\n  Classification Report:\")\nprint(classification_report(y_test, lr_pred, target_names=['Loss', 'Win']))\n\n# Confusion matrix\ncm = confusion_matrix(y_test, lr_pred)\nprint(f\"\\n  Confusion Matrix:\")\nprint(f\"    True Negatives: {cm[0,0]:,}  |  False Positives: {cm[0,1]:,}\")\nprint(f\"    False Negatives: {cm[1,0]:,} |  True Positives: {cm[1,1]:,}\")"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Model 2: Random Forest"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "# Train Random Forest\nprint(\"Training Random Forest...\")\nrf_model = RandomForestClassifier(n_estimators=100, random_state=42, n_jobs=-1, max_depth=10)\nrf_model.fit(X_train, y_train)\n\n# Predictions\nrf_pred = rf_model.predict(X_test)\nrf_pred_proba = rf_model.predict_proba(X_test)[:, 1]\n\n# Evaluation metrics\nrf_acc = accuracy_score(y_test, rf_pred)\nrf_roc_auc = roc_auc_score(y_test, rf_pred_proba)\n\nprint(f\"\\nâœ“ Random Forest Results:\")\nprint(f\"  Accuracy: {rf_acc:.4f} ({rf_acc*100:.2f}%)\")\nprint(f\"  ROC-AUC: {rf_roc_auc:.4f}\")\nprint(f\"\\n  Classification Report:\")\nprint(classification_report(y_test, rf_pred, target_names=['Loss', 'Win']))\n\n# Confusion matrix\ncm = confusion_matrix(y_test, rf_pred)\nprint(f\"\\n  Confusion Matrix:\")\nprint(f\"    True Negatives: {cm[0,0]:,}  |  False Positives: {cm[0,1]:,}\")\nprint(f\"    False Negatives: {cm[1,0]:,} |  True Positives: {cm[1,1]:,}\")\n\n# Store feature importances for later analysis\nrf_feature_importance = pd.DataFrame({\n    'feature': X.columns,\n    'importance': rf_model.feature_importances_\n}).sort_values('importance', ascending=False)"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6. Model 3: XGBoost"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "# Train XGBoost\nprint(\"Training XGBoost...\")\nxgb_model = xgb.XGBClassifier(\n    n_estimators=100, \n    learning_rate=0.1, \n    max_depth=6,\n    random_state=42,\n    n_jobs=-1,\n    eval_metric='logloss'\n)\nxgb_model.fit(X_train, y_train)\n\n# Predictions\nxgb_pred = xgb_model.predict(X_test)\nxgb_pred_proba = xgb_model.predict_proba(X_test)[:, 1]\n\n# Evaluation metrics\nxgb_acc = accuracy_score(y_test, xgb_pred)\nxgb_roc_auc = roc_auc_score(y_test, xgb_pred_proba)\n\nprint(f\"\\nâœ“ XGBoost Results:\")\nprint(f\"  Accuracy: {xgb_acc:.4f} ({xgb_acc*100:.2f}%)\")\nprint(f\"  ROC-AUC: {xgb_roc_auc:.4f}\")\nprint(f\"\\n  Classification Report:\")\nprint(classification_report(y_test, xgb_pred, target_names=['Loss', 'Win']))\n\n# Confusion matrix\ncm = confusion_matrix(y_test, xgb_pred)\nprint(f\"\\n  Confusion Matrix:\")\nprint(f\"    True Negatives: {cm[0,0]:,}  |  False Positives: {cm[0,1]:,}\")\nprint(f\"    False Negatives: {cm[1,0]:,} |  True Positives: {cm[1,1]:,}\")\n\n# Store feature importances for later analysis\nxgb_feature_importance = pd.DataFrame({\n    'feature': X.columns,\n    'importance': xgb_model.feature_importances_\n}).sort_values('importance', ascending=False)"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 7. Feature Importance Analysis\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "# Visualize feature importance from best model (use Random Forest)\nfig, (ax1, ax2) = plt.subplots(1, 2, figsize=(16, 6))\n\n# Random Forest feature importance\ntop_rf = rf_feature_importance.head(15)\nax1.barh(top_rf['feature'], top_rf['importance'], color='steelblue', edgecolor='black', alpha=0.8)\nax1.set_xlabel('Importance Score', fontsize=14)\nax1.set_title('Top 15 Features: Random Forest', fontsize=16, fontweight='bold')\nax1.invert_yaxis()\nax1.grid(axis='x', alpha=0.3)\n\n# XGBoost feature importance\ntop_xgb = xgb_feature_importance.head(15)\nax2.barh(top_xgb['feature'], top_xgb['importance'], color='coral', edgecolor='black', alpha=0.8)\nax2.set_xlabel('Importance Score', fontsize=14)\nax2.set_title('Top 15 Features: XGBoost', fontsize=16, fontweight='bold')\nax2.invert_yaxis()\nax2.grid(axis='x', alpha=0.3)\n\nplt.tight_layout()\nplt.show()\n\nprint(\"\\nðŸ“Š Top 10 Most Important Features (Random Forest):\")\nprint(rf_feature_importance.head(10).to_string(index=False))\n\nprint(\"\\nðŸ’¡ Key Insights:\")\nprint(f\"  - Most important feature: {rf_feature_importance.iloc[0]['feature']}\")\nprint(f\"  - Top 3 features account for {rf_feature_importance.head(3)['importance'].sum()*100:.1f}% of total importance\")\nprint(f\"  - These features tell us what matters most for winning battles!\")"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 8. Model Evaluation Summary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "# Create comprehensive model comparison\nresults = pd.DataFrame({\n    'Model': ['Logistic Regression', 'Random Forest', 'XGBoost'],\n    'Accuracy': [lr_acc, rf_acc, xgb_acc],\n    'ROC-AUC': [lr_roc_auc, rf_roc_auc, xgb_roc_auc]\n})\n\n# Add percentage formatting\nresults['Accuracy (%)'] = results['Accuracy'] * 100\nresults['ROC-AUC (%)'] = results['ROC-AUC'] * 100\n\nprint(\"=\" * 70)\nprint(\"ðŸ“Š MODEL PERFORMANCE SUMMARY\")\nprint(\"=\" * 70)\nprint(results[['Model', 'Accuracy (%)', 'ROC-AUC (%)']].to_string(index=False))\nprint(\"=\" * 70)\n\n# Identify best model\nbest_model_idx = results['Accuracy'].idxmax()\nbest_model_name = results.loc[best_model_idx, 'Model']\nbest_acc = results.loc[best_model_idx, 'Accuracy (%)']\n\nprint(f\"\\nðŸ† BEST MODEL: {best_model_name}\")\nprint(f\"   Accuracy: {best_acc:.2f}%\")\nprint(f\"   ROC-AUC: {results.loc[best_model_idx, 'ROC-AUC (%)']:.2f}%\")\n\n# Compare to benchmark\nbenchmark_acc = 56.94\nprint(f\"\\nðŸ“ˆ Comparison to Research Benchmark:\")\nprint(f\"   Previous research: {benchmark_acc}%\")\nprint(f\"   Our best model: {best_acc:.2f}%\")\nif best_acc > benchmark_acc:\n    print(f\"   âœ… We beat the benchmark by {best_acc - benchmark_acc:.2f} percentage points!\")\nelse:\n    print(f\"   âš  We're {benchmark_acc - best_acc:.2f} percentage points below the benchmark\")\n\nprint(f\"\\nðŸ’¡ Key Takeaways for Presentation:\")\nprint(f\"   1. Achieved {best_acc:.1f}% accuracy predicting battle outcomes\")\nprint(f\"   2. {best_model_name} performed best among 3 models tested\")\nprint(f\"   3. Feature importance shows {rf_feature_importance.iloc[0]['feature']} is most critical\")\nprint(f\"   4. Model demonstrates deck composition has predictive power\")"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Insights for Presentation\n",
    "\n",
    "**Key Points**:\n",
    "1. Achieved X% accuracy (compare to 56.94% benchmark)\n",
    "2. Top 3 most important features are: [list]\n",
    "3. This means: [actionable insight from feature importance]"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}