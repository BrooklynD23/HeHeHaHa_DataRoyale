{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 06 - Modeling: Battle Outcome Prediction\n",
    "\n",
    "**Purpose**: Build predictive models for technical rigor scoring.\n",
    "\n",
    "**Goal**: Predict battle outcomes based on deck composition alone.\n",
    "\n",
    "**Models to Try**:\n",
    "1. Logistic Regression (baseline)\n",
    "2. Random Forest (feature importance insights)\n",
    "3. XGBoost (likely best performance)\n",
    "\n",
    "**Key Metrics**:\n",
    "- Accuracy\n",
    "- Precision/Recall\n",
    "- ROC-AUC\n",
    "- Feature importance (for insights!)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✓ Presentation style configured\n"
     ]
    }
   ],
   "source": [
    "import sys, os, pandas as pd, numpy as np\n",
    "import matplotlib.pyplot as plt, seaborn as sns\n",
    "\n",
    "from sklearn.model_selection import train_test_split, cross_val_score\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.metrics import accuracy_score, classification_report, roc_auc_score, confusion_matrix\n",
    "import xgboost as xgb\n",
    "\n",
    "PROJECT_ROOT = os.path.abspath(os.path.join(os.getcwd(), '..'))\n",
    "sys.path.insert(0, os.path.join(PROJECT_ROOT, 'src'))\n",
    "\n",
    "from visualization import setup_presentation_style\n",
    "setup_presentation_style()"
   ]
  },
  {
   "cell_type": "code",
   "source": "# Import GPU detection utilities\nPROJECT_ROOT = os.path.abspath(os.path.join(os.getcwd(), '..'))\nsys.path.insert(0, os.path.join(PROJECT_ROOT, 'src'))\n\nfrom system_utils import get_xgboost_params, print_cuda_info, configure_environment_for_ml\n\n# Configure ML environment (auto-detects GPU)\nml_config = configure_environment_for_ml(verbose=True)",
   "metadata": {},
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": "# Load engineered features from notebook 05\nfeatures_path = os.path.join(PROJECT_ROOT, 'artifacts/model_features.parquet')\n\nif not os.path.exists(features_path):\n    print(\"❌ ERROR: model_features.parquet not found!\")\n    print(\"   Please run Notebook 05 first to create features.\")\n    raise FileNotFoundError(f\"Missing: {features_path}\")\n\nfeatures = pd.read_parquet(features_path)\n\nprint(f\"✓ Loaded {len(features):,} battles with {len(features.columns)} features\")\nprint(f\"  Memory usage: {features.memory_usage(deep=True).sum() / 1024**2:.1f} MB\")"
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "ename": "FileNotFoundError",
     "evalue": "[Errno 2] No such file or directory: 'c:\\\\Users\\\\Danny\\\\Documents\\\\GitHub\\\\HeHeHaHa_DataRoyale\\\\artifacts/model_features.parquet'",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mFileNotFoundError\u001b[39m                         Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[2]\u001b[39m\u001b[32m, line 2\u001b[39m\n\u001b[32m      1\u001b[39m \u001b[38;5;66;03m# Load engineered features from notebook 05\u001b[39;00m\n\u001b[32m----> \u001b[39m\u001b[32m2\u001b[39m features = \u001b[43mpd\u001b[49m\u001b[43m.\u001b[49m\u001b[43mread_parquet\u001b[49m\u001b[43m(\u001b[49m\u001b[43mos\u001b[49m\u001b[43m.\u001b[49m\u001b[43mpath\u001b[49m\u001b[43m.\u001b[49m\u001b[43mjoin\u001b[49m\u001b[43m(\u001b[49m\u001b[43mPROJECT_ROOT\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[33;43m'\u001b[39;49m\u001b[33;43martifacts/model_features.parquet\u001b[39;49m\u001b[33;43m'\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m      4\u001b[39m \u001b[38;5;28mprint\u001b[39m(\u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[33mLoaded \u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mlen\u001b[39m(features)\u001b[38;5;132;01m:\u001b[39;00m\u001b[33m,\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m battles with \u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mlen\u001b[39m(features.columns)\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m features\u001b[39m\u001b[33m\"\u001b[39m)\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\Danny\\AppData\\Local\\Programs\\Python\\Python313\\Lib\\site-packages\\pandas\\io\\parquet.py:669\u001b[39m, in \u001b[36mread_parquet\u001b[39m\u001b[34m(path, engine, columns, storage_options, use_nullable_dtypes, dtype_backend, filesystem, filters, **kwargs)\u001b[39m\n\u001b[32m    666\u001b[39m     use_nullable_dtypes = \u001b[38;5;28;01mFalse\u001b[39;00m\n\u001b[32m    667\u001b[39m check_dtype_backend(dtype_backend)\n\u001b[32m--> \u001b[39m\u001b[32m669\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mimpl\u001b[49m\u001b[43m.\u001b[49m\u001b[43mread\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m    670\u001b[39m \u001b[43m    \u001b[49m\u001b[43mpath\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    671\u001b[39m \u001b[43m    \u001b[49m\u001b[43mcolumns\u001b[49m\u001b[43m=\u001b[49m\u001b[43mcolumns\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    672\u001b[39m \u001b[43m    \u001b[49m\u001b[43mfilters\u001b[49m\u001b[43m=\u001b[49m\u001b[43mfilters\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    673\u001b[39m \u001b[43m    \u001b[49m\u001b[43mstorage_options\u001b[49m\u001b[43m=\u001b[49m\u001b[43mstorage_options\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    674\u001b[39m \u001b[43m    \u001b[49m\u001b[43muse_nullable_dtypes\u001b[49m\u001b[43m=\u001b[49m\u001b[43muse_nullable_dtypes\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    675\u001b[39m \u001b[43m    \u001b[49m\u001b[43mdtype_backend\u001b[49m\u001b[43m=\u001b[49m\u001b[43mdtype_backend\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    676\u001b[39m \u001b[43m    \u001b[49m\u001b[43mfilesystem\u001b[49m\u001b[43m=\u001b[49m\u001b[43mfilesystem\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    677\u001b[39m \u001b[43m    \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    678\u001b[39m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\Danny\\AppData\\Local\\Programs\\Python\\Python313\\Lib\\site-packages\\pandas\\io\\parquet.py:258\u001b[39m, in \u001b[36mPyArrowImpl.read\u001b[39m\u001b[34m(self, path, columns, filters, use_nullable_dtypes, dtype_backend, storage_options, filesystem, **kwargs)\u001b[39m\n\u001b[32m    256\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m manager == \u001b[33m\"\u001b[39m\u001b[33marray\u001b[39m\u001b[33m\"\u001b[39m:\n\u001b[32m    257\u001b[39m     to_pandas_kwargs[\u001b[33m\"\u001b[39m\u001b[33msplit_blocks\u001b[39m\u001b[33m\"\u001b[39m] = \u001b[38;5;28;01mTrue\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m258\u001b[39m path_or_handle, handles, filesystem = \u001b[43m_get_path_or_handle\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m    259\u001b[39m \u001b[43m    \u001b[49m\u001b[43mpath\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    260\u001b[39m \u001b[43m    \u001b[49m\u001b[43mfilesystem\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    261\u001b[39m \u001b[43m    \u001b[49m\u001b[43mstorage_options\u001b[49m\u001b[43m=\u001b[49m\u001b[43mstorage_options\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    262\u001b[39m \u001b[43m    \u001b[49m\u001b[43mmode\u001b[49m\u001b[43m=\u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mrb\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[32m    263\u001b[39m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    264\u001b[39m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[32m    265\u001b[39m     pa_table = \u001b[38;5;28mself\u001b[39m.api.parquet.read_table(\n\u001b[32m    266\u001b[39m         path_or_handle,\n\u001b[32m    267\u001b[39m         columns=columns,\n\u001b[32m   (...)\u001b[39m\u001b[32m    270\u001b[39m         **kwargs,\n\u001b[32m    271\u001b[39m     )\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\Danny\\AppData\\Local\\Programs\\Python\\Python313\\Lib\\site-packages\\pandas\\io\\parquet.py:141\u001b[39m, in \u001b[36m_get_path_or_handle\u001b[39m\u001b[34m(path, fs, storage_options, mode, is_dir)\u001b[39m\n\u001b[32m    131\u001b[39m handles = \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[32m    132\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m (\n\u001b[32m    133\u001b[39m     \u001b[38;5;129;01mnot\u001b[39;00m fs\n\u001b[32m    134\u001b[39m     \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m is_dir\n\u001b[32m   (...)\u001b[39m\u001b[32m    139\u001b[39m     \u001b[38;5;66;03m# fsspec resources can also point to directories\u001b[39;00m\n\u001b[32m    140\u001b[39m     \u001b[38;5;66;03m# this branch is used for example when reading from non-fsspec URLs\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m141\u001b[39m     handles = \u001b[43mget_handle\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m    142\u001b[39m \u001b[43m        \u001b[49m\u001b[43mpath_or_handle\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmode\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mis_text\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43;01mFalse\u001b[39;49;00m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mstorage_options\u001b[49m\u001b[43m=\u001b[49m\u001b[43mstorage_options\u001b[49m\n\u001b[32m    143\u001b[39m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    144\u001b[39m     fs = \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[32m    145\u001b[39m     path_or_handle = handles.handle\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\Danny\\AppData\\Local\\Programs\\Python\\Python313\\Lib\\site-packages\\pandas\\io\\common.py:882\u001b[39m, in \u001b[36mget_handle\u001b[39m\u001b[34m(path_or_buf, mode, encoding, compression, memory_map, is_text, errors, storage_options)\u001b[39m\n\u001b[32m    873\u001b[39m         handle = \u001b[38;5;28mopen\u001b[39m(\n\u001b[32m    874\u001b[39m             handle,\n\u001b[32m    875\u001b[39m             ioargs.mode,\n\u001b[32m   (...)\u001b[39m\u001b[32m    878\u001b[39m             newline=\u001b[33m\"\u001b[39m\u001b[33m\"\u001b[39m,\n\u001b[32m    879\u001b[39m         )\n\u001b[32m    880\u001b[39m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m    881\u001b[39m         \u001b[38;5;66;03m# Binary mode\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m882\u001b[39m         handle = \u001b[38;5;28;43mopen\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mhandle\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mioargs\u001b[49m\u001b[43m.\u001b[49m\u001b[43mmode\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    883\u001b[39m     handles.append(handle)\n\u001b[32m    885\u001b[39m \u001b[38;5;66;03m# Convert BytesIO or file objects passed with an encoding\u001b[39;00m\n",
      "\u001b[31mFileNotFoundError\u001b[39m: [Errno 2] No such file or directory: 'c:\\\\Users\\\\Danny\\\\Documents\\\\GitHub\\\\HeHeHaHa_DataRoyale\\\\artifacts/model_features.parquet'"
     ]
    }
   ],
   "source": [
    "# Load engineered features from notebook 05\n",
    "features = pd.read_parquet(os.path.join(PROJECT_ROOT, 'artifacts/model_features.parquet'))\n",
    "\n",
    "print(f\"Loaded {len(features):,} battles with {len(features.columns)} features\")"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": "# Restructure data: each battle becomes 2 rows (one for each player)\n# Target variable: 1 if player won, 0 if lost\n\nprint(\"Restructuring data for binary classification...\")\n\n# Create winner rows (outcome = 1)\nwinner_data = features.copy()\nwinner_data['outcome'] = 1\n\n# Create loser rows (outcome = 0) - need to swap winner/loser columns\nloser_data = features.copy()\nloser_data['outcome'] = 0\n\n# For simplicity, we'll just use winner's features to predict winner's victory\n# Select numeric features only\nnumeric_cols = features.select_dtypes(include=[np.number]).columns.tolist()\n\n# Define feature columns (exclude target-like columns)\nexclude_cols = ['outcome', 'winner.trophyChange', 'loser.trophyChange', \n                'winner.crowns', 'loser.crowns']\nfeature_cols = [col for col in numeric_cols if not any(ex in col for ex in exclude_cols)]\n\n# Prepare X and y\nX = winner_data[feature_cols].fillna(0)\ny = winner_data['outcome']\n\nprint(f\"✓ Data prepared for modeling:\")\nprint(f\"  Samples: {len(X):,}\")\nprint(f\"  Features: {len(feature_cols)}\")\nprint(f\"  Target distribution: {y.value_counts().to_dict()}\")\n\n# Show sample features\nprint(f\"\\n  Sample features (first 10):\")\nfor col in feature_cols[:10]:\n    print(f\"    - {col}\")"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO: Define target variable (1 = winner won, 0 = loser won - always 1 in this dataset!)\n",
    "# Need to restructure: each battle becomes 2 rows (one for each player)\n",
    "# with outcome = 1 if that player won, 0 if lost\n",
    "\n",
    "# Example structure:\n",
    "# y = features['outcome']  # 1 or 0\n",
    "# X = features[feature_columns]  # numeric features only\n",
    "\n",
    "print(\"TODO: Restructure data and select features\")"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": "# Create train/test split with stratification\nprint(\"Creating train/test split (80/20)...\")\n\nX_train, X_test, y_train, y_test = train_test_split(\n    X, y, test_size=0.2, random_state=42, stratify=y\n)\n\nprint(f\"✓ Split complete:\")\nprint(f\"  Training samples: {len(X_train):,}\")\nprint(f\"  Testing samples: {len(X_test):,}\")\nprint(f\"  Training class distribution: {y_train.value_counts().to_dict()}\")\nprint(f\"  Testing class distribution: {y_test.value_counts().to_dict()}\")"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO: Split data\n",
    "# X_train, X_test, y_train, y_test = train_test_split(\n",
    "#     X, y, test_size=0.2, random_state=42, stratify=y\n",
    "# )\n",
    "\n",
    "print(\"TODO: Create train/test split\")"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": "# Train logistic regression baseline model\nprint(\"Training Logistic Regression (baseline)...\")\n\nlr_model = LogisticRegression(max_iter=1000, random_state=42, n_jobs=-1)\nlr_model.fit(X_train, y_train)\n\n# Predictions\nlr_pred = lr_model.predict(X_test)\nlr_pred_proba = lr_model.predict_proba(X_test)[:, 1]\n\n# Metrics\nlr_acc = accuracy_score(y_test, lr_pred)\nlr_auc = roc_auc_score(y_test, lr_pred_proba)\n\nprint(f\"✓ Logistic Regression trained:\")\nprint(f\"  Accuracy: {lr_acc:.4f}\")\nprint(f\"  ROC-AUC: {lr_auc:.4f}\")\nprint(f\"\\nClassification Report:\")\nprint(classification_report(y_test, lr_pred))"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO: Train logistic regression\n",
    "# lr_model = LogisticRegression(max_iter=1000, random_state=42)\n",
    "# lr_model.fit(X_train, y_train)\n",
    "# lr_pred = lr_model.predict(X_test)\n",
    "# lr_acc = accuracy_score(y_test, lr_pred)\n",
    "# print(f\"Logistic Regression Accuracy: {lr_acc:.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": "# Train random forest for feature importance insights\nprint(\"Training Random Forest...\")\n\nrf_model = RandomForestClassifier(n_estimators=100, max_depth=10, random_state=42, n_jobs=-1)\nrf_model.fit(X_train, y_train)\n\n# Predictions\nrf_pred = rf_model.predict(X_test)\nrf_pred_proba = rf_model.predict_proba(X_test)[:, 1]\n\n# Metrics\nrf_acc = accuracy_score(y_test, rf_pred)\nrf_auc = roc_auc_score(y_test, rf_pred_proba)\n\nprint(f\"✓ Random Forest trained:\")\nprint(f\"  Accuracy: {rf_acc:.4f}\")\nprint(f\"  ROC-AUC: {rf_auc:.4f}\")\nprint(f\"\\nClassification Report:\")\nprint(classification_report(y_test, rf_pred))"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO: Train random forest\n",
    "# rf_model = RandomForestClassifier(n_estimators=100, random_state=42, n_jobs=-1)\n",
    "# rf_model.fit(X_train, y_train)\n",
    "# rf_pred = rf_model.predict(X_test)\n",
    "# rf_acc = accuracy_score(y_test, rf_pred)\n",
    "# print(f\"Random Forest Accuracy: {rf_acc:.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": "# Train XGBoost with auto GPU detection\nprint(\"Training XGBoost (with GPU support if available)...\")\n\n# Get optimal XGBoost parameters for this machine\nxgb_params = get_xgboost_params()\nprint(f\"  Using configuration: {xgb_params}\")\n\nxgb_model = xgb.XGBClassifier(\n    **xgb_params,\n    n_estimators=100,\n    learning_rate=0.1,\n    max_depth=6,\n    random_state=42\n)\n\nxgb_model.fit(X_train, y_train)\n\n# Predictions\nxgb_pred = xgb_model.predict(X_test)\nxgb_pred_proba = xgb_model.predict_proba(X_test)[:, 1]\n\n# Metrics\nxgb_acc = accuracy_score(y_test, xgb_pred)\nxgb_auc = roc_auc_score(y_test, xgb_pred_proba)\n\nprint(f\"\\n✓ XGBoost trained:\")\nprint(f\"  Accuracy: {xgb_acc:.4f}\")\nprint(f\"  ROC-AUC: {xgb_auc:.4f}\")\nprint(f\"\\nClassification Report:\")\nprint(classification_report(y_test, xgb_pred))"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO: Train XGBoost\n",
    "# xgb_model = xgb.XGBClassifier(n_estimators=100, learning_rate=0.1, random_state=42)\n",
    "# xgb_model.fit(X_train, y_train)\n",
    "# xgb_pred = xgb_model.predict(X_test)\n",
    "# xgb_acc = accuracy_score(y_test, xgb_pred)\n",
    "# print(f\"XGBoost Accuracy: {xgb_acc:.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": "# Extract feature importances from Random Forest and XGBoost\nprint(\"Analyzing feature importance...\")\n\n# Random Forest importances\nrf_importances = pd.DataFrame({\n    'feature': feature_cols,\n    'importance': rf_model.feature_importances_\n}).sort_values('importance', ascending=False)\n\n# XGBoost importances\nxgb_importances = pd.DataFrame({\n    'feature': feature_cols,\n    'importance': xgb_model.feature_importances_\n}).sort_values('importance', ascending=False)\n\n# Visualization\nfig, axes = plt.subplots(1, 2, figsize=(18, 8))\n\n# Random Forest top 15\nax = axes[0]\ntop_rf = rf_importances.head(15)\nax.barh(range(len(top_rf)), top_rf['importance'], color='forestgreen', edgecolor='black', alpha=0.8)\nax.set_yticks(range(len(top_rf)))\nax.set_yticklabels(top_rf['feature'], fontsize=10)\nax.set_xlabel('Feature Importance', fontsize=12)\nax.set_title('Top 15 Features: Random Forest', fontsize=14, fontweight='bold')\nax.invert_yaxis()\nax.grid(axis='x', alpha=0.3)\n\n# XGBoost top 15\nax = axes[1]\ntop_xgb = xgb_importances.head(15)\nax.barh(range(len(top_xgb)), top_xgb['importance'], color='steelblue', edgecolor='black', alpha=0.8)\nax.set_yticks(range(len(top_xgb)))\nax.set_yticklabels(top_xgb['feature'], fontsize=10)\nax.set_xlabel('Feature Importance', fontsize=12)\nax.set_title('Top 15 Features: XGBoost', fontsize=14, fontweight='bold')\nax.invert_yaxis()\nax.grid(axis='x', alpha=0.3)\n\nplt.tight_layout()\nplt.savefig(os.path.join(PROJECT_ROOT, 'presentation/figures/fig_feature_importance.png'),\n            dpi=300, bbox_inches='tight')\nplt.show()\n\nprint(\"\\n✓ Top 10 Most Important Features (XGBoost):\")\nfor idx, row in xgb_importances.head(10).iterrows():\n    print(f\"  {row['feature']}: {row['importance']:.4f}\")\n\n# Save importances\nrf_importances.to_parquet(os.path.join(PROJECT_ROOT, 'artifacts/rf_feature_importance.parquet'), index=False)\nxgb_importances.to_parquet(os.path.join(PROJECT_ROOT, 'artifacts/xgb_feature_importance.parquet'), index=False)"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO: Extract feature importances from best model\n",
    "# Plot top 15 most important features\n",
    "# These tell the story of what matters most for winning!"
   ]
  },
  {
   "cell_type": "code",
   "source": "# Create summary table and comparison visualization\nprint(\"Creating model comparison...\")\n\nresults = pd.DataFrame({\n    'Model': ['Logistic Regression', 'Random Forest', 'XGBoost'],\n    'Accuracy': [lr_acc, rf_acc, xgb_acc],\n    'ROC-AUC': [lr_auc, rf_auc, xgb_auc]\n})\n\n# Visualization\nfig, axes = plt.subplots(1, 2, figsize=(16, 6))\n\n# Accuracy comparison\nax = axes[0]\ncolors = ['coral', 'forestgreen', 'steelblue']\nax.bar(results['Model'], results['Accuracy'], color=colors, edgecolor='black', alpha=0.8)\nax.set_ylabel('Accuracy', fontsize=14)\nax.set_title('Model Accuracy Comparison', fontsize=16, fontweight='bold')\nax.set_ylim([0.5, 1.0])\nax.grid(axis='y', alpha=0.3)\nfor i, (model, acc) in enumerate(zip(results['Model'], results['Accuracy'])):\n    ax.text(i, acc + 0.01, f'{acc:.4f}', ha='center', fontsize=12, fontweight='bold')\n\n# ROC-AUC comparison\nax = axes[1]\nax.bar(results['Model'], results['ROC-AUC'], color=colors, edgecolor='black', alpha=0.8)\nax.set_ylabel('ROC-AUC', fontsize=14)\nax.set_title('Model ROC-AUC Comparison', fontsize=16, fontweight='bold')\nax.set_ylim([0.5, 1.0])\nax.grid(axis='y', alpha=0.3)\nfor i, (model, auc) in enumerate(zip(results['Model'], results['ROC-AUC'])):\n    ax.text(i, auc + 0.01, f'{auc:.4f}', ha='center', fontsize=12, fontweight='bold')\n\nplt.tight_layout()\nplt.savefig(os.path.join(PROJECT_ROOT, 'presentation/figures/fig4_model_comparison.png'),\n            dpi=300, bbox_inches='tight')\nplt.show()\n\nprint(\"\\n✓ Model Comparison Summary:\")\nprint(results.to_string(index=False))\n\n# Identify best model\nbest_model_idx = results['ROC-AUC'].idxmax()\nbest_model = results.loc[best_model_idx, 'Model']\nprint(f\"\\n✓ Best performing model: {best_model}\")\nprint(f\"  Accuracy: {results.loc[best_model_idx, 'Accuracy']:.4f}\")\nprint(f\"  ROC-AUC: {results.loc[best_model_idx, 'ROC-AUC']:.4f}\")",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "source": "# Model calibration analysis\nfrom sklearn.calibration import calibration_curve\nfrom sklearn.metrics import brier_score_loss\n\nprint(\"Analyzing model calibration...\")\n\n# Calculate calibration curves\nlr_prob_true, lr_prob_pred = calibration_curve(y_test, lr_pred_proba, n_bins=10)\nrf_prob_true, rf_prob_pred = calibration_curve(y_test, rf_pred_proba, n_bins=10)\nxgb_prob_true, xgb_prob_pred = calibration_curve(y_test, xgb_pred_proba, n_bins=10)\n\n# Calculate Brier scores (lower is better)\nlr_brier = brier_score_loss(y_test, lr_pred_proba)\nrf_brier = brier_score_loss(y_test, rf_pred_proba)\nxgb_brier = brier_score_loss(y_test, xgb_pred_proba)\n\n# Visualization\nfig, ax = plt.subplots(figsize=(10, 10))\n\n# Perfect calibration line\nax.plot([0, 1], [0, 1], 'k--', linewidth=2, label='Perfect Calibration')\n\n# Model calibration curves\nax.plot(lr_prob_pred, lr_prob_true, marker='o', linewidth=2, label=f'Logistic Regression (Brier: {lr_brier:.4f})')\nax.plot(rf_prob_pred, rf_prob_true, marker='s', linewidth=2, label=f'Random Forest (Brier: {rf_brier:.4f})')\nax.plot(xgb_prob_pred, xgb_prob_true, marker='^', linewidth=2, label=f'XGBoost (Brier: {xgb_brier:.4f})')\n\nax.set_xlabel('Mean Predicted Probability', fontsize=14)\nax.set_ylabel('Fraction of Positives', fontsize=14)\nax.set_title('Model Calibration Curves', fontsize=16, fontweight='bold', pad=15)\nax.legend(fontsize=12)\nax.grid(alpha=0.3)\nax.set_xlim([0, 1])\nax.set_ylim([0, 1])\n\nplt.tight_layout()\nplt.savefig(os.path.join(PROJECT_ROOT, 'presentation/figures/fig_model_calibration.png'),\n            dpi=300, bbox_inches='tight')\nplt.show()\n\nprint(\"\\n✓ Calibration Analysis:\")\nprint(f\"  Logistic Regression Brier Score: {lr_brier:.4f}\")\nprint(f\"  Random Forest Brier Score: {rf_brier:.4f}\")\nprint(f\"  XGBoost Brier Score: {xgb_brier:.4f}\")\nprint(f\"\\n  Best calibrated model: {['Logistic Regression', 'Random Forest', 'XGBoost'][np.argmin([lr_brier, rf_brier, xgb_brier])]}\")",
   "metadata": {},
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "source": "## 10. Save Models & Artifacts",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "source": "# Save trained models and metrics\nimport joblib\n\nprint(\"Saving models and artifacts...\")\n\n# Save models\nmodels_dir = os.path.join(PROJECT_ROOT, 'artifacts/models')\nos.makedirs(models_dir, exist_ok=True)\n\njoblib.dump(lr_model, os.path.join(models_dir, 'logistic_regression.pkl'))\njoblib.dump(rf_model, os.path.join(models_dir, 'random_forest.pkl'))\njoblib.dump(xgb_model, os.path.join(models_dir, 'xgboost.pkl'))\n\nprint(f\"✓ Models saved to {models_dir}/\")\n\n# Save metrics summary\nmetrics_summary = pd.DataFrame({\n    'Model': ['Logistic Regression', 'Random Forest', 'XGBoost'],\n    'Accuracy': [lr_acc, rf_acc, xgb_acc],\n    'ROC-AUC': [lr_auc, rf_auc, xgb_auc],\n    'Brier Score': [lr_brier, rf_brier, xgb_brier]\n})\n\nmetrics_summary.to_csv(os.path.join(PROJECT_ROOT, 'artifacts/model_metrics_summary.csv'), index=False)\nprint(\"✓ Metrics summary saved to artifacts/model_metrics_summary.csv\")",
   "metadata": {},
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 8. Model Evaluation Summary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO: Create summary table\n",
    "# results = pd.DataFrame({\n",
    "#     'Model': ['Logistic Regression', 'Random Forest', 'XGBoost'],\n",
    "#     'Accuracy': [lr_acc, rf_acc, xgb_acc],\n",
    "#     'ROC-AUC': [...]\n",
    "# })\n",
    "\n",
    "print(\"TODO: Summarize model performance\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Insights for Presentation\n",
    "\n",
    "**Key Points**:\n",
    "1. Achieved X% accuracy (compare to 56.94% benchmark)\n",
    "2. Top 3 most important features are: [list]\n",
    "3. This means: [actionable insight from feature importance]"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}