{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 04.5 - Advanced Meta Analysis & Actionable Insights\n",
    "\n",
    "**Purpose**: Extract comprehensive, actionable insights for competitive play\n",
    "\n",
    "**Key Questions**:\n",
    "- What are the optimal deck characteristics (elixir, composition)?\n",
    "- Which specific cards and card combos dominate the meta?\n",
    "- How does the meta evolve across trophy progression walls?\n",
    "- What do our ML models tell us about winning strategies?\n",
    "\n",
    "**Unique Features**:\n",
    "- Data-driven trophy wall detection (not hardcoded)\n",
    "- Card synergy analysis (2-card combos)\n",
    "- Evolution card impact assessment\n",
    "- Statistical confidence intervals on all metrics\n",
    "- Archetype performance by trophy level\n",
    "- Card level impact analysis\n",
    "\n",
    "**Output**: Presentation-ready insights with human-readable card names"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys, os, json\n",
    "import duckdb, pandas as pd, numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from scipy import stats\n",
    "from itertools import combinations\n",
    "\n",
    "PROJECT_ROOT = os.path.abspath(os.path.join(os.getcwd(), '..'))\n",
    "sys.path.insert(0, os.path.join(PROJECT_ROOT, 'src'))\n",
    "\n",
    "# Use Parquet if available\n",
    "DATA_PATH = os.path.join(PROJECT_ROOT, 'battles.parquet')\n",
    "if not os.path.exists(DATA_PATH):\n",
    "    DATA_PATH = os.path.join(PROJECT_ROOT, 'battles.csv')\n",
    "\n",
    "from duckdb_utils import get_connection, create_battles_view, query_to_df, save_to_parquet\n",
    "from visualization import setup_presentation_style\n",
    "\n",
    "con = get_connection()\n",
    "create_battles_view(con, DATA_PATH)\n",
    "setup_presentation_style()\n",
    "\n",
    "# Ensure artifacts and figures directories exist\n",
    "os.makedirs(os.path.join(PROJECT_ROOT, 'artifacts'), exist_ok=True)\n",
    "os.makedirs(os.path.join(PROJECT_ROOT, 'presentation/figures'), exist_ok=True)\n",
    "\n",
    "print(\"âœ“ Environment setup complete\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Section 0: Load Card Name Mapping\n",
    "\n",
    "Convert card IDs to human-readable names for all analyses"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Try to load cards.json from multiple locations\n",
    "card_mapping = {}\n",
    "card_file_locations = [\n",
    "    os.path.join(PROJECT_ROOT, 'artifacts', 'cards.json'),\n",
    "    os.path.join(PROJECT_ROOT, 'Datasets', 'cards.json'),\n",
    "    os.path.join(PROJECT_ROOT, 'cards.json')\n",
    "]\n",
    "\n",
    "cards_loaded = False\n",
    "for card_file in card_file_locations:\n",
    "    if os.path.exists(card_file):\n",
    "        print(f\"Found cards.json at: {card_file}\")\n",
    "        with open(card_file, 'r') as f:\n",
    "            card_data = json.load(f)\n",
    "            \n",
    "            # Handle different JSON structures\n",
    "            if isinstance(card_data, dict):\n",
    "                # If already a mapping\n",
    "                card_mapping = {int(k) if k.isdigit() else k: v for k, v in card_data.items()}\n",
    "            elif isinstance(card_data, list):\n",
    "                # If it's a list of card objects\n",
    "                for card in card_data:\n",
    "                    if 'id' in card and 'name' in card:\n",
    "                        card_mapping[card['id']] = card['name']\n",
    "            \n",
    "            cards_loaded = True\n",
    "            print(f\"âœ“ Loaded {len(card_mapping)} card mappings\")\n",
    "            break\n",
    "\n",
    "if not cards_loaded:\n",
    "    print(\"âš  Warning: cards.json not found. Will use card IDs instead of names.\")\n",
    "    print(\"  Expected locations:\")\n",
    "    for loc in card_file_locations:\n",
    "        print(f\"    - {loc}\")\n",
    "\n",
    "def get_card_name(card_id):\n",
    "    \"\"\"Convert card ID to name, with fallback to ID if not found\"\"\"\n",
    "    if pd.isna(card_id):\n",
    "        return None\n",
    "    \n",
    "    # Try as integer\n",
    "    try:\n",
    "        card_id_int = int(card_id)\n",
    "        if card_id_int in card_mapping:\n",
    "            return card_mapping[card_id_int]\n",
    "    except (ValueError, TypeError):\n",
    "        pass\n",
    "    \n",
    "    # Try as string\n",
    "    if card_id in card_mapping:\n",
    "        return card_mapping[card_id]\n",
    "    \n",
    "    # Fallback to ID\n",
    "    return f\"Card_{card_id}\"\n",
    "\n",
    "def is_evolution_card(card_name):\n",
    "    \"\"\"Detect if a card is an evolution variant\"\"\"\n",
    "    if not card_name:\n",
    "        return False\n",
    "    evolution_keywords = ['evolved', 'evo', 'evolution', 'evolved ']\n",
    "    return any(keyword in str(card_name).lower() for keyword in evolution_keywords)\n",
    "\n",
    "# Test the mapping\n",
    "if cards_loaded:\n",
    "    sample_ids = list(card_mapping.keys())[:5]\n",
    "    print(f\"\\nSample card mappings:\")\n",
    "    for card_id in sample_ids:\n",
    "        name = get_card_name(card_id)\n",
    "        evo = \"[EVO]\" if is_evolution_card(name) else \"\"\n",
    "        print(f\"  {card_id} â†’ {name} {evo}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Section 1: Data-Driven Trophy Wall Detection\n",
    "\n",
    "Instead of assuming 4k/5k/6k/7k, let's detect walls from battle density"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Query battle distribution to find natural clustering points\n",
    "trophy_dist_query = \"\"\"\n",
    "SELECT \n",
    "    FLOOR(\"average.startingTrophies\" / 100) * 100 as trophy_bin,\n",
    "    COUNT(*) as battle_count\n",
    "FROM battles\n",
    "WHERE \"average.startingTrophies\" IS NOT NULL\n",
    "    AND \"average.startingTrophies\" BETWEEN 0 AND 10000\n",
    "GROUP BY trophy_bin\n",
    "ORDER BY trophy_bin\n",
    "\"\"\"\n",
    "\n",
    "trophy_distribution = query_to_df(con, trophy_dist_query, show_progress=False)\n",
    "\n",
    "print(f\"Trophy distribution loaded: {len(trophy_distribution)} bins\")\n",
    "print(f\"Trophy range: {trophy_distribution['trophy_bin'].min():.0f} to {trophy_distribution['trophy_bin'].max():.0f}\")\n",
    "print(f\"Total battles: {trophy_distribution['battle_count'].sum():,}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "# Detect trophy walls using peak detection\nfrom scipy.signal import find_peaks\n\n# Normalize battle counts for peak detection\nnormalized_counts = trophy_distribution['battle_count'] / trophy_distribution['battle_count'].max()\n\n# Find peaks (local maxima) - these are trophy walls\npeaks, properties = find_peaks(normalized_counts, prominence=0.1, distance=10)\n\ndetected_walls = trophy_distribution.iloc[peaks]['trophy_bin'].values\n\n# Use standard walls if detection fails or finds too few\nstandard_walls = [4000, 5000, 6000, 7000]\nif len(detected_walls) < 3:\n    print(\"âš  Using standard trophy walls: 4k, 5k, 6k, 7k\")\n    trophy_walls = standard_walls\nelse:\n    # Round detected walls to nearest 1000\n    trophy_walls = [int(round(w, -3)) for w in detected_walls if 3000 <= w <= 8000]\n    # Ensure we have key milestones\n    for wall in standard_walls:\n        if not any(abs(w - wall) < 500 for w in trophy_walls):\n            trophy_walls.append(wall)\n    trophy_walls = sorted(set(trophy_walls))\n\nprint(f\"\\nâœ“ Trophy Walls Detected: {trophy_walls}\")\n\n# Visualize distribution with detected walls\nfig, ax = plt.subplots(figsize=(14, 7))\n\nax.bar(trophy_distribution['trophy_bin'], trophy_distribution['battle_count'], \n       width=90, color='steelblue', edgecolor='black', alpha=0.7)\n\n# Mark detected walls\ncolors = ['red', 'orange', 'purple', 'darkred', 'maroon']\nfor i, wall in enumerate(trophy_walls[:5]):\n    color = colors[i] if i < len(colors) else 'black'\n    ax.axvline(wall, color=color, linestyle='--', linewidth=2.5, alpha=0.8,\n               label=f'{wall/1000:.0f}k Trophy Wall')\n\nax.set_xlabel('Trophy Count', fontsize=16)\nax.set_ylabel('Number of Battles', fontsize=16)\nax.set_title('Trophy Distribution: Data-Driven Wall Detection', fontsize=20, fontweight='bold', pad=20)\nax.legend(fontsize=12, loc='upper right')\nax.grid(axis='y', alpha=0.3)\n\nplt.tight_layout()\nplt.savefig(os.path.join(PROJECT_ROOT, 'presentation/figures/fig5_detected_walls.png'), \n            dpi=300, bbox_inches='tight')\nplt.show()\n\n# Define trophy brackets based on detected walls\ntrophy_brackets = {}\nwalls_with_bounds = [0] + trophy_walls + [10000]\nfor i in range(len(walls_with_bounds) - 1):\n    lower = walls_with_bounds[i]\n    upper = walls_with_bounds[i + 1]\n    if lower == 0:\n        label = f'0-{upper/1000:.0f}k'\n    elif upper == 10000:\n        label = f'{lower/1000:.0f}k+'\n    else:\n        label = f'{lower/1000:.0f}k-{upper/1000:.0f}k'\n    trophy_brackets[label] = (lower, upper)\n\nprint(f\"\\nðŸ“Š Trophy Brackets Defined:\")\nfor label, (low, high) in trophy_brackets.items():\n    count = trophy_distribution[(trophy_distribution['trophy_bin'] >= low) & \n                                 (trophy_distribution['trophy_bin'] < high)]['battle_count'].sum()\n    print(f\"  {label:<15} ({low:>5} - {high:>5}): {count:>12,} battles\")\n\n# Save detected walls\ndetected_walls_data = {\n    'walls': trophy_walls,\n    'brackets': {k: list(v) for k, v in trophy_brackets.items()}\n}\nwith open(os.path.join(PROJECT_ROOT, 'artifacts/detected_trophy_walls.json'), 'w') as f:\n    json.dump(detected_walls_data, f, indent=2)\n\nprint(f\"\\nâœ“ Trophy walls saved to artifacts/detected_trophy_walls.json\")"
  },
  {
   "cell_type": "code",
   "source": "# 2.1 Optimal Elixir Cost Analysis\nelixir_query = \"\"\"\nSELECT \n    ROUND(\"winner.elixir.average\" * 4) / 4 as elixir_bucket,\n    COUNT(*) as battles,\n    SUM(CASE WHEN \"winner.trophyChange\" > 0 THEN 1 ELSE 0 END) as wins,\n    AVG(\"winner.startingTrophies\") as avg_trophies\nFROM battles\nWHERE \"winner.elixir.average\" IS NOT NULL\n    AND \"winner.elixir.average\" BETWEEN 2.0 AND 6.0\nGROUP BY elixir_bucket\nHAVING battles > 1000\nORDER BY elixir_bucket\n\"\"\"\n\nelixir_data = query_to_df(con, elixir_query, show_progress=False)\nelixir_data['win_rate'] = elixir_data['wins'] / elixir_data['battles']\n\n# Calculate 95% confidence intervals\ndef wilson_confidence_interval(wins, total, z=1.96):\n    \"\"\"Wilson score interval for binomial proportion\"\"\"\n    if total == 0:\n        return 0, 0\n    p_hat = wins / total\n    denominator = 1 + z**2 / total\n    center = (p_hat + z**2 / (2 * total)) / denominator\n    margin = z * np.sqrt((p_hat * (1 - p_hat) + z**2 / (4 * total)) / total) / denominator\n    return center - margin, center + margin\n\nelixir_data['ci_lower'], elixir_data['ci_upper'] = zip(*elixir_data.apply(\n    lambda row: wilson_confidence_interval(row['wins'], row['battles']), axis=1))\n\n# Find optimal elixir range\noptimal_elixir = elixir_data.loc[elixir_data['win_rate'].idxmax(), 'elixir_bucket']\noptimal_wr = elixir_data['win_rate'].max()\n\n# Visualization\nfig, ax = plt.subplots(figsize=(14, 7))\n\nax.plot(elixir_data['elixir_bucket'], elixir_data['win_rate'] * 100, \n        marker='o', linewidth=3, markersize=10, color='steelblue', label='Win Rate')\nax.fill_between(elixir_data['elixir_bucket'], \n                elixir_data['ci_lower'] * 100, \n                elixir_data['ci_upper'] * 100,\n                alpha=0.3, color='steelblue', label='95% Confidence Interval')\n\n# Mark optimal point\nax.axvline(optimal_elixir, color='red', linestyle='--', linewidth=2, alpha=0.7)\nax.axhline(optimal_wr * 100, color='red', linestyle=':', linewidth=1.5, alpha=0.5)\nax.scatter([optimal_elixir], [optimal_wr * 100], color='red', s=200, zorder=5, \n           label=f'Optimal: {optimal_elixir:.2f} elixir')\n\nax.set_xlabel('Average Elixir Cost', fontsize=16)\nax.set_ylabel('Win Rate (%)', fontsize=16)\nax.set_title('Optimal Elixir Cost for Winning Decks', fontsize=20, fontweight='bold', pad=20)\nax.legend(fontsize=12)\nax.grid(alpha=0.3)\nax.set_ylim(48, 54)\n\nplt.tight_layout()\nplt.savefig(os.path.join(PROJECT_ROOT, 'presentation/figures/fig7_optimal_elixir.png'),\n            dpi=300, bbox_inches='tight')\nplt.show()\n\nprint(f\"\\\\nâœ“ Optimal Elixir Analysis:\")\nprint(f\"  Best elixir cost: {optimal_elixir:.2f} ({optimal_wr*100:.2f}% win rate)\")\nprint(f\"  95% CI: [{elixir_data.loc[elixir_data['win_rate'].idxmax(), 'ci_lower']*100:.2f}%, {elixir_data.loc[elixir_data['win_rate'].idxmax(), 'ci_upper']*100:.2f}%]\")\nprint(f\"  Sample size: {elixir_data.loc[elixir_data['win_rate'].idxmax(), 'battles']:,} battles\")\n\n# Identify optimal range (within 1% of best)\noptimal_range = elixir_data[elixir_data['win_rate'] >= optimal_wr - 0.01]\nprint(f\"  Optimal range: {optimal_range['elixir_bucket'].min():.2f} - {optimal_range['elixir_bucket'].max():.2f} elixir\")",
   "metadata": {},
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "source": "## Section 2: Optimal Deck Characteristics\n\nFind the \"Goldilocks zone\" for deck building with statistical confidence",
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": "### 2.2 Optimal Card Rarity Distribution\n\nAnalyze win rates by legendary/epic/rare/common composition",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "source": "# Analyze win rates by rarity composition\nrarity_query = \"\"\"\nSELECT \n    \"winner.rarity.legendary\" as legendary_count,\n    \"winner.rarity.epic\" as epic_count,\n    \"winner.rarity.rare\" as rare_count,\n    \"winner.rarity.common\" as common_count,\n    COUNT(*) as battles,\n    SUM(CASE WHEN \"winner.trophyChange\" > 0 THEN 1 ELSE 0 END) as wins\nFROM battles\nWHERE \"winner.rarity.legendary\" IS NOT NULL\nGROUP BY legendary_count, epic_count, rare_count, common_count\nHAVING battles > 500\n\"\"\"\n\nrarity_data = query_to_df(con, rarity_query, show_progress=False)\nrarity_data['win_rate'] = rarity_data['wins'] / rarity_data['battles']\n\n# Calculate confidence intervals\nrarity_data['ci_lower'], rarity_data['ci_upper'] = zip(*rarity_data.apply(\n    lambda row: wilson_confidence_interval(row['wins'], row['battles']), axis=1))\n\n# Analyze by legendary count (most interesting)\nlegendary_analysis = rarity_data.groupby('legendary_count').agg({\n    'battles': 'sum',\n    'wins': 'sum'\n}).reset_index()\nlegendary_analysis['win_rate'] = legendary_analysis['wins'] / legendary_analysis['battles']\nlegendary_analysis['ci_lower'], legendary_analysis['ci_upper'] = zip(*legendary_analysis.apply(\n    lambda row: wilson_confidence_interval(row['wins'], row['battles']), axis=1))\n\n# Visualization\nfig, axes = plt.subplots(1, 2, figsize=(16, 6))\n\n# Left: Win rate by legendary count\nax = axes[0]\nax.bar(legendary_analysis['legendary_count'], legendary_analysis['win_rate'] * 100,\n       color='gold', edgecolor='black', alpha=0.7)\nax.errorbar(legendary_analysis['legendary_count'], legendary_analysis['win_rate'] * 100,\n            yerr=[(legendary_analysis['win_rate'] - legendary_analysis['ci_lower']) * 100,\n                  (legendary_analysis['ci_upper'] - legendary_analysis['win_rate']) * 100],\n            fmt='none', color='black', capsize=5, linewidth=2)\nax.set_xlabel('Number of Legendary Cards in Deck', fontsize=14)\nax.set_ylabel('Win Rate (%)', fontsize=14)\nax.set_title('Win Rate by Legendary Card Count', fontsize=16, fontweight='bold')\nax.grid(axis='y', alpha=0.3)\n\n# Right: Battle distribution\nax = axes[1]\nax.bar(legendary_analysis['legendary_count'], legendary_analysis['battles'],\n       color='steelblue', edgecolor='black', alpha=0.7)\nax.set_xlabel('Number of Legendary Cards', fontsize=14)\nax.set_ylabel('Number of Battles', fontsize=14)\nax.set_title('Deck Composition Distribution', fontsize=16, fontweight='bold')\nax.grid(axis='y', alpha=0.3)\n\nplt.tight_layout()\nplt.savefig(os.path.join(PROJECT_ROOT, 'presentation/figures/fig8_rarity_analysis.png'),\n            dpi=300, bbox_inches='tight')\nplt.show()\n\n# Find optimal legendary count\noptimal_legendary = legendary_analysis.loc[legendary_analysis['win_rate'].idxmax()]\nprint(f\"\\nâœ“ Optimal Rarity Analysis:\")\nprint(f\"  Best legendary count: {optimal_legendary['legendary_count']:.0f} ({optimal_legendary['win_rate']*100:.2f}% WR)\")\nprint(f\"  95% CI: [{optimal_legendary['ci_lower']*100:.2f}%, {optimal_legendary['ci_upper']*100:.2f}%]\")\nprint(f\"  Sample size: {optimal_legendary['battles']:,.0f} battles\")",
   "metadata": {},
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "source": "### 2.3 Optimal Troop/Spell/Structure Balance\n\nAnalyze the ideal composition of card types",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "source": "# Analyze win rates by card type composition\ncomposition_query = \"\"\"\nSELECT \n    \"winner.troop.count\" as troop_count,\n    \"winner.spell.count\" as spell_count,\n    \"winner.structure.count\" as structure_count,\n    COUNT(*) as battles,\n    SUM(CASE WHEN \"winner.trophyChange\" > 0 THEN 1 ELSE 0 END) as wins\nFROM battles\nWHERE \"winner.troop.count\" IS NOT NULL\n    AND \"winner.spell.count\" IS NOT NULL\n    AND \"winner.structure.count\" IS NOT NULL\nGROUP BY troop_count, spell_count, structure_count\nHAVING battles > 200\n\"\"\"\n\ncomposition_data = query_to_df(con, composition_query, show_progress=False)\ncomposition_data['win_rate'] = composition_data['wins'] / composition_data['battles']\n\n# Calculate confidence intervals\ncomposition_data['ci_lower'], composition_data['ci_upper'] = zip(*composition_data.apply(\n    lambda row: wilson_confidence_interval(row['wins'], row['battles']), axis=1))\n\n# Find top compositions\ntop_compositions = composition_data.nlargest(10, 'win_rate')[\n    ['troop_count', 'spell_count', 'structure_count', 'win_rate', 'battles']\n].copy()\ntop_compositions['composition'] = (\n    top_compositions['troop_count'].astype(str) + 'T-' +\n    top_compositions['spell_count'].astype(str) + 'Sp-' +\n    top_compositions['structure_count'].astype(str) + 'St'\n)\n\n# Visualization\nfig, ax = plt.subplots(figsize=(14, 7))\n\ncolors = plt.cm.viridis(np.linspace(0, 1, len(top_compositions)))\nbars = ax.barh(range(len(top_compositions)), top_compositions['win_rate'] * 100,\n               color=colors, edgecolor='black', alpha=0.8)\n\nax.set_yticks(range(len(top_compositions)))\nax.set_yticklabels(top_compositions['composition'])\nax.set_xlabel('Win Rate (%)', fontsize=14)\nax.set_ylabel('Deck Composition', fontsize=14)\nax.set_title('Top 10 Card Type Compositions by Win Rate', fontsize=16, fontweight='bold', pad=15)\nax.axvline(50, color='red', linestyle='--', linewidth=1, alpha=0.5, label='50% baseline')\nax.legend()\nax.grid(axis='x', alpha=0.3)\n\nplt.tight_layout()\nplt.savefig(os.path.join(PROJECT_ROOT, 'presentation/figures/fig9_composition_balance.png'),\n            dpi=300, bbox_inches='tight')\nplt.show()\n\nprint(\"\\nâœ“ Top 5 Deck Compositions:\")\nfor idx, row in top_compositions.head(5).iterrows():\n    print(f\"  {row['composition']}: {row['win_rate']*100:.2f}% WR ({row['battles']:.0f} battles)\")",
   "metadata": {},
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "source": "### 2.4 Statistical Significance Testing\n\nVerify that observed differences are statistically significant",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "source": "# Chi-square test for deck composition differences\nfrom scipy.stats import chi2_contingency, ttest_ind\n\n# Test if elixir cost significantly affects win rate\n# Compare high elixir (>4.0) vs low elixir (<3.5) decks\nhigh_elixir = elixir_data[elixir_data['elixir_bucket'] > 4.0]\nlow_elixir = elixir_data[elixir_data['elixir_bucket'] < 3.5]\n\nif len(high_elixir) > 0 and len(low_elixir) > 0:\n    # Proportion test using chi-square\n    contingency_table = np.array([\n        [high_elixir['wins'].sum(), high_elixir['battles'].sum() - high_elixir['wins'].sum()],\n        [low_elixir['wins'].sum(), low_elixir['battles'].sum() - low_elixir['wins'].sum()]\n    ])\n    \n    chi2, p_value, dof, expected = chi2_contingency(contingency_table)\n    \n    print(\"âœ“ Statistical Significance Tests:\")\n    print(f\"\\n1. Elixir Cost Impact (High >4.0 vs Low <3.5):\")\n    print(f\"   Chi-square statistic: {chi2:.2f}\")\n    print(f\"   P-value: {p_value:.2e}\")\n    print(f\"   Result: {'SIGNIFICANT' if p_value < 0.05 else 'NOT SIGNIFICANT'} at Î±=0.05\")\n    print(f\"   High elixir WR: {high_elixir['wins'].sum()/high_elixir['battles'].sum()*100:.2f}%\")\n    print(f\"   Low elixir WR: {low_elixir['wins'].sum()/low_elixir['battles'].sum()*100:.2f}%\")\n\n# Test legendary count impact\nif len(legendary_analysis) >= 2:\n    # Compare 0 legendaries vs 2+ legendaries\n    zero_leg = legendary_analysis[legendary_analysis['legendary_count'] == 0]\n    high_leg = legendary_analysis[legendary_analysis['legendary_count'] >= 2]\n    \n    if len(zero_leg) > 0 and len(high_leg) > 0:\n        contingency_table_leg = np.array([\n            [zero_leg['wins'].sum(), zero_leg['battles'].sum() - zero_leg['wins'].sum()],\n            [high_leg['wins'].sum(), high_leg['battles'].sum() - high_leg['wins'].sum()]\n        ])\n        \n        chi2_leg, p_value_leg, _, _ = chi2_contingency(contingency_table_leg)\n        \n        print(f\"\\n2. Legendary Count Impact (0 vs 2+):\")\n        print(f\"   Chi-square statistic: {chi2_leg:.2f}\")\n        print(f\"   P-value: {p_value_leg:.2e}\")\n        print(f\"   Result: {'SIGNIFICANT' if p_value_leg < 0.05 else 'NOT SIGNIFICANT'} at Î±=0.05\")\n        print(f\"   0 legendaries WR: {zero_leg['wins'].sum()/zero_leg['battles'].sum()*100:.2f}%\")\n        print(f\"   2+ legendaries WR: {high_leg['wins'].sum()/high_leg['battles'].sum()*100:.2f}%\")\n\nprint(\"\\nâœ“ All deck characteristic differences tested for statistical significance\")",
   "metadata": {},
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "source": "## Section 3: Card Synergy Analysis\n\nIdentify which 2-card combinations work best together using lift metrics",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "source": "# Extract all card pairs from winner decks\n# Query to get individual card win rates first\ncard_wr_query = \"\"\"\nSELECT \n    card_id,\n    COUNT(*) as usage,\n    SUM(CASE WHEN \"winner.trophyChange\" > 0 THEN 1 ELSE 0 END) as wins\nFROM (\n    SELECT \"winner.card1.id\" as card_id, \"winner.trophyChange\" FROM battles WHERE \"winner.card1.id\" IS NOT NULL\n    UNION ALL SELECT \"winner.card2.id\", \"winner.trophyChange\" FROM battles WHERE \"winner.card2.id\" IS NOT NULL\n    UNION ALL SELECT \"winner.card3.id\", \"winner.trophyChange\" FROM battles WHERE \"winner.card3.id\" IS NOT NULL\n    UNION ALL SELECT \"winner.card4.id\", \"winner.trophyChange\" FROM battles WHERE \"winner.card4.id\" IS NOT NULL\n    UNION ALL SELECT \"winner.card5.id\", \"winner.trophyChange\" FROM battles WHERE \"winner.card5.id\" IS NOT NULL\n    UNION ALL SELECT \"winner.card6.id\", \"winner.trophyChange\" FROM battles WHERE \"winner.card6.id\" IS NOT NULL\n    UNION ALL SELECT \"winner.card7.id\", \"winner.trophyChange\" FROM battles WHERE \"winner.card7.id\" IS NOT NULL\n    UNION ALL SELECT \"winner.card8.id\", \"winner.trophyChange\" FROM battles WHERE \"winner.card8.id\" IS NOT NULL\n) \nGROUP BY card_id\nHAVING usage > 1000\n\"\"\"\n\nprint(\"Calculating individual card win rates...\")\ncard_win_rates = query_to_df(con, card_wr_query, show_progress=False)\ncard_win_rates['win_rate'] = card_win_rates['wins'] / card_win_rates['usage']\n\n# Create card win rate lookup\ncard_wr_dict = dict(zip(card_win_rates['card_id'], card_win_rates['win_rate']))\n\nprint(f\"âœ“ Loaded {len(card_win_rates)} cards with >1000 usage\")",
   "metadata": {},
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "source": "# Calculate 2-card synergies\n# Sample to speed up - use 20% of battles\nprint(\"Calculating 2-card synergies (this may take a few minutes)...\")\n\nsynergy_query = \"\"\"\nWITH card_pairs AS (\n    SELECT \n        LEAST(\"winner.card1.id\", \"winner.card2.id\") as card1,\n        GREATEST(\"winner.card1.id\", \"winner.card2.id\") as card2,\n        \"winner.trophyChange\" > 0 as won\n    FROM battles\n    WHERE \"winner.card1.id\" IS NOT NULL AND \"winner.card2.id\" IS NOT NULL\n        AND RANDOM() < 0.2\n    UNION ALL\n    SELECT LEAST(\"winner.card1.id\", \"winner.card3.id\"), GREATEST(\"winner.card1.id\", \"winner.card3.id\"), \"winner.trophyChange\" > 0 FROM battles WHERE \"winner.card1.id\" IS NOT NULL AND \"winner.card3.id\" IS NOT NULL AND RANDOM() < 0.2\n    UNION ALL\n    SELECT LEAST(\"winner.card1.id\", \"winner.card4.id\"), GREATEST(\"winner.card1.id\", \"winner.card4.id\"), \"winner.trophyChange\" > 0 FROM battles WHERE \"winner.card1.id\" IS NOT NULL AND \"winner.card4.id\" IS NOT NULL AND RANDOM() < 0.2\n    UNION ALL\n    SELECT LEAST(\"winner.card2.id\", \"winner.card3.id\"), GREATEST(\"winner.card2.id\", \"winner.card3.id\"), \"winner.trophyChange\" > 0 FROM battles WHERE \"winner.card2.id\" IS NOT NULL AND \"winner.card3.id\" IS NOT NULL AND RANDOM() < 0.2\n    UNION ALL\n    SELECT LEAST(\"winner.card2.id\", \"winner.card4.id\"), GREATEST(\"winner.card2.id\", \"winner.card4.id\"), \"winner.trophyChange\" > 0 FROM battles WHERE \"winner.card2.id\" IS NOT NULL AND \"winner.card4.id\" IS NOT NULL AND RANDOM() < 0.2\n    UNION ALL\n    SELECT LEAST(\"winner.card3.id\", \"winner.card4.id\"), GREATEST(\"winner.card3.id\", \"winner.card4.id\"), \"winner.trophyChange\" > 0 FROM battles WHERE \"winner.card3.id\" IS NOT NULL AND \"winner.card4.id\" IS NOT NULL AND RANDOM() < 0.2\n    UNION ALL\n    SELECT LEAST(\"winner.card5.id\", \"winner.card6.id\"), GREATEST(\"winner.card5.id\", \"winner.card6.id\"), \"winner.trophyChange\" > 0 FROM battles WHERE \"winner.card5.id\" IS NOT NULL AND \"winner.card6.id\" IS NOT NULL AND RANDOM() < 0.2\n    UNION ALL\n    SELECT LEAST(\"winner.card7.id\", \"winner.card8.id\"), GREATEST(\"winner.card7.id\", \"winner.card8.id\"), \"winner.trophyChange\" > 0 FROM battles WHERE \"winner.card7.id\" IS NOT NULL AND \"winner.card8.id\" IS NOT NULL AND RANDOM() < 0.2\n)\nSELECT \n    card1,\n    card2,\n    COUNT(*) as pair_usage,\n    SUM(CASE WHEN won THEN 1 ELSE 0 END) as pair_wins\nFROM card_pairs\nGROUP BY card1, card2\nHAVING pair_usage > 100\n\"\"\"\n\npair_data = query_to_df(con, synergy_query, show_progress=False)\npair_data['pair_wr'] = pair_data['pair_wins'] / pair_data['pair_usage']\n\n# Calculate lift metric: actual_wr / (card1_wr * card2_wr)\n# Lift > 1 means synergy, < 1 means anti-synergy\npair_data['card1_wr'] = pair_data['card1'].map(card_wr_dict)\npair_data['card2_wr'] = pair_data['card2'].map(card_wr_dict)\npair_data = pair_data.dropna(subset=['card1_wr', 'card2_wr'])\npair_data['expected_wr'] = pair_data['card1_wr'] * pair_data['card2_wr']\npair_data['lift'] = pair_data['pair_wr'] / pair_data['expected_wr']\n\n# Add card names\npair_data['card1_name'] = pair_data['card1'].apply(get_card_name)\npair_data['card2_name'] = pair_data['card2'].apply(get_card_name)\n\nprint(f\"âœ“ Analyzed {len(pair_data):,} card pairs with >100 usage\")",
   "metadata": {},
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "source": "# Find top synergies and anti-synergies\ntop_synergies = pair_data.nlargest(20, 'lift')[['card1_name', 'card2_name', 'lift', 'pair_wr', 'pair_usage']].copy()\ntop_anti = pair_data.nsmallest(20, 'lift')[['card1_name', 'card2_name', 'lift', 'pair_wr', 'pair_usage']].copy()\n\n# Visualization\nfig, axes = plt.subplots(1, 2, figsize=(18, 8))\n\n# Top Synergies\nax = axes[0]\ntop_synergies_plot = top_synergies.head(15).copy()\ntop_synergies_plot['pair_label'] = top_synergies_plot['card1_name'] + ' + ' + top_synergies_plot['card2_name']\ncolors_syn = plt.cm.Greens(np.linspace(0.4, 0.9, len(top_synergies_plot)))\nax.barh(range(len(top_synergies_plot)), top_synergies_plot['lift'], color=colors_syn, edgecolor='black', alpha=0.8)\nax.set_yticks(range(len(top_synergies_plot)))\nax.set_yticklabels(top_synergies_plot['pair_label'], fontsize=10)\nax.set_xlabel('Lift (Actual WR / Expected WR)', fontsize=12)\nax.set_title('Top 15 Card Synergies', fontsize=14, fontweight='bold')\nax.axvline(1.0, color='red', linestyle='--', linewidth=1.5, alpha=0.7, label='No Effect (Lift=1.0)')\nax.legend()\nax.grid(axis='x', alpha=0.3)\n\n# Top Anti-Synergies\nax = axes[1]\ntop_anti_plot = top_anti.head(15).copy()\ntop_anti_plot['pair_label'] = top_anti_plot['card1_name'] + ' + ' + top_anti_plot['card2_name']\ncolors_anti = plt.cm.Reds(np.linspace(0.4, 0.9, len(top_anti_plot)))\nax.barh(range(len(top_anti_plot)), top_anti_plot['lift'], color=colors_anti, edgecolor='black', alpha=0.8)\nax.set_yticks(range(len(top_anti_plot)))\nax.set_yticklabels(top_anti_plot['pair_label'], fontsize=10)\nax.set_xlabel('Lift (Actual WR / Expected WR)', fontsize=12)\nax.set_title('Top 15 Anti-Synergies (Avoid These)', fontsize=14, fontweight='bold')\nax.axvline(1.0, color='green', linestyle='--', linewidth=1.5, alpha=0.7, label='No Effect (Lift=1.0)')\nax.legend()\nax.grid(axis='x', alpha=0.3)\n\nplt.tight_layout()\nplt.savefig(os.path.join(PROJECT_ROOT, 'presentation/figures/fig10_card_synergies.png'),\n            dpi=300, bbox_inches='tight')\nplt.show()\n\n# Print top findings\nprint(\"\\nâœ“ Top 5 Card Synergies:\")\nfor idx, row in top_synergies.head(5).iterrows():\n    print(f\"  {row['card1_name']} + {row['card2_name']}: Lift={row['lift']:.3f} ({row['pair_usage']:.0f} uses)\")\n\nprint(\"\\nâœ“ Top 5 Anti-Synergies (Avoid):\")\nfor idx, row in top_anti.head(5).iterrows():\n    print(f\"  {row['card1_name']} + {row['card2_name']}: Lift={row['lift']:.3f} ({row['pair_usage']:.0f} uses)\")\n\n# Save synergy data\nsave_to_parquet(pair_data, 'artifacts/card_synergies.parquet')\nprint(\"\\nâœ“ Synergy data saved to artifacts/card_synergies.parquet\")",
   "metadata": {},
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "source": "## Section 4: Meta Card Rankings\n\nRank individual cards by win rate, usage, and card level impact",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "source": "# Analyze individual card performance with more detail\ncard_rankings = card_win_rates.copy()\ncard_rankings['card_name'] = card_rankings['card_id'].apply(get_card_name)\ncard_rankings['is_evolution'] = card_rankings['card_name'].apply(is_evolution_card)\n\n# Sort by win rate\ncard_rankings = card_rankings.sort_values('win_rate', ascending=False)\n\n# Categorize by win rate\ncard_rankings['tier'] = pd.cut(card_rankings['win_rate'], \n                                bins=[0, 0.48, 0.50, 0.52, 1.0],\n                                labels=['Weak', 'Below Average', 'Above Average', 'Strong'])\n\n# Visualization: Top and bottom 20 cards\nfig, axes = plt.subplots(1, 2, figsize=(18, 10))\n\n# Top 20\nax = axes[0]\ntop_cards = card_rankings.head(20)\ncolors_top = plt.cm.RdYlGn(np.linspace(0.5, 1, len(top_cards)))\nax.barh(range(len(top_cards)), top_cards['win_rate'] * 100, color=colors_top, edgecolor='black', alpha=0.8)\nax.set_yticks(range(len(top_cards)))\nax.set_yticklabels(top_cards['card_name'], fontsize=10)\nax.set_xlabel('Win Rate (%)', fontsize=12)\nax.set_title('Top 20 Cards by Win Rate', fontsize=14, fontweight='bold')\nax.axvline(50, color='black', linestyle='--', linewidth=1, alpha=0.5)\nax.grid(axis='x', alpha=0.3)\n\n# Bottom 20\nax = axes[1]\nbottom_cards = card_rankings.tail(20)\ncolors_bottom = plt.cm.RdYlGn(np.linspace(0, 0.5, len(bottom_cards)))\nax.barh(range(len(bottom_cards)), bottom_cards['win_rate'] * 100, color=colors_bottom, edgecolor='black', alpha=0.8)\nax.set_yticks(range(len(bottom_cards)))\nax.set_yticklabels(bottom_cards['card_name'], fontsize=10)\nax.set_xlabel('Win Rate (%)', fontsize=12)\nax.set_title('Bottom 20 Cards by Win Rate', fontsize=14, fontweight='bold')\nax.axvline(50, color='black', linestyle='--', linewidth=1, alpha=0.5)\nax.grid(axis='x', alpha=0.3)\n\nplt.tight_layout()\nplt.savefig(os.path.join(PROJECT_ROOT, 'presentation/figures/fig11_card_rankings.png'),\n            dpi=300, bbox_inches='tight')\nplt.show()\n\n# Print tier summary\nprint(\"\\nâœ“ Card Tier Distribution:\")\nprint(card_rankings['tier'].value_counts().sort_index())\n\nprint(\"\\nâœ“ Top 10 Cards:\")\nfor idx, row in card_rankings.head(10).iterrows():\n    evo_tag = \" [EVO]\" if row['is_evolution'] else \"\"\n    print(f\"  {row['card_name']}{evo_tag}: {row['win_rate']*100:.2f}% WR ({row['usage']:,} uses)\")\n\n# Save rankings\nsave_to_parquet(card_rankings, 'artifacts/card_meta_rankings.parquet')\nprint(\"\\nâœ“ Card rankings saved to artifacts/card_meta_rankings.parquet\")",
   "metadata": {},
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "source": "## Section 5: Trophy-Specific Meta Analysis\n\nAnalyze how top cards and strategies change across trophy walls",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "source": "# Analyze meta by trophy bracket\n# Create case statement for trophy brackets\nbracket_cases = []\nfor label, (low, high) in trophy_brackets.items():\n    bracket_cases.append(f\"WHEN \\\"average.startingTrophies\\\" >= {low} AND \\\"average.startingTrophies\\\" < {high} THEN '{label}'\")\n\nbracket_case_sql = \"CASE \" + \" \".join(bracket_cases) + \" END\"\n\ntrophy_meta_query = f\"\"\"\nSELECT \n    {bracket_case_sql} as bracket,\n    card_id,\n    COUNT(*) as usage,\n    SUM(CASE WHEN \"winner.trophyChange\" > 0 THEN 1 ELSE 0 END) as wins\nFROM (\n    SELECT {bracket_case_sql} as bracket, \"winner.card1.id\" as card_id, \"winner.trophyChange\", \"average.startingTrophies\" FROM battles WHERE \"winner.card1.id\" IS NOT NULL\n    UNION ALL SELECT {bracket_case_sql}, \"winner.card2.id\", \"winner.trophyChange\", \"average.startingTrophies\" FROM battles WHERE \"winner.card2.id\" IS NOT NULL\n    UNION ALL SELECT {bracket_case_sql}, \"winner.card3.id\", \"winner.trophyChange\", \"average.startingTrophies\" FROM battles WHERE \"winner.card3.id\" IS NOT NULL\n    UNION ALL SELECT {bracket_case_sql}, \"winner.card4.id\", \"winner.trophyChange\", \"average.startingTrophies\" FROM battles WHERE \"winner.card4.id\" IS NOT NULL\n    UNION ALL SELECT {bracket_case_sql}, \"winner.card5.id\", \"winner.trophyChange\", \"average.startingTrophies\" FROM battles WHERE \"winner.card5.id\" IS NOT NULL\n) \nWHERE bracket IS NOT NULL\nGROUP BY bracket, card_id\nHAVING usage > 500\n\"\"\"\n\nprint(\"Analyzing trophy-specific meta (this may take a few minutes)...\")\ntrophy_meta = query_to_df(con, trophy_meta_query, show_progress=False)\ntrophy_meta['win_rate'] = trophy_meta['wins'] / trophy_meta['usage']\ntrophy_meta['card_name'] = trophy_meta['card_id'].apply(get_card_name)\n\n# Get top 5 cards per bracket\ntop_cards_by_bracket = {}\nfor bracket in trophy_brackets.keys():\n    bracket_data = trophy_meta[trophy_meta['bracket'] == bracket].nlargest(5, 'win_rate')\n    top_cards_by_bracket[bracket] = bracket_data[['card_name', 'win_rate', 'usage']].values.tolist()\n\nprint(f\"\\nâœ“ Trophy-Specific Meta Analysis Complete\")\nprint(\"\\nTop 5 Cards by Trophy Bracket:\")\nfor bracket, cards in top_cards_by_bracket.items():\n    print(f\"\\n  {bracket}:\")\n    for card_name, wr, usage in cards[:3]:\n        print(f\"    {card_name}: {wr*100:.2f}% WR ({usage:.0f} uses)\")\n\n# Visualization: Heatmap of top cards across brackets\ntop_cards_global = card_rankings.head(15)['card_name'].values\nheatmap_data = []\nfor card in top_cards_global:\n    row = []\n    for bracket in trophy_brackets.keys():\n        bracket_data = trophy_meta[(trophy_meta['bracket'] == bracket) & (trophy_meta['card_name'] == card)]\n        if len(bracket_data) > 0:\n            row.append(bracket_data.iloc[0]['win_rate'] * 100)\n        else:\n            row.append(np.nan)\n    heatmap_data.append(row)\n\nfig, ax = plt.subplots(figsize=(14, 10))\nim = ax.imshow(heatmap_data, cmap='RdYlGn', aspect='auto', vmin=48, vmax=54)\nax.set_xticks(range(len(trophy_brackets)))\nax.set_xticklabels(list(trophy_brackets.keys()), rotation=45, ha='right')\nax.set_yticks(range(len(top_cards_global)))\nax.set_yticklabels(top_cards_global)\nax.set_xlabel('Trophy Bracket', fontsize=14)\nax.set_ylabel('Card', fontsize=14)\nax.set_title('Top Cards Win Rate Across Trophy Brackets', fontsize=16, fontweight='bold', pad=15)\n\n# Add colorbar\ncbar = plt.colorbar(im, ax=ax)\ncbar.set_label('Win Rate (%)', fontsize=12)\n\n# Add text annotations\nfor i in range(len(top_cards_global)):\n    for j in range(len(trophy_brackets)):\n        if not np.isnan(heatmap_data[i][j]):\n            text = ax.text(j, i, f'{heatmap_data[i][j]:.1f}',\n                          ha=\"center\", va=\"center\", color=\"black\", fontsize=8)\n\nplt.tight_layout()\nplt.savefig(os.path.join(PROJECT_ROOT, 'presentation/figures/fig12_trophy_meta_heatmap.png'),\n            dpi=300, bbox_inches='tight')\nplt.show()\n\n# Save trophy meta\nsave_to_parquet(trophy_meta, 'artifacts/trophy_specific_meta.parquet')\nprint(\"\\nâœ“ Trophy meta saved to artifacts/trophy_specific_meta.parquet\")",
   "metadata": {},
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "source": "## Section 6: Deck Archetype Analysis\n\nCluster decks into archetypes and analyze matchup performance",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "source": "# Define simple archetype rules based on deck characteristics\narchetype_query = \"\"\"\nSELECT \n    \"winner.elixir.average\" as elixir_avg,\n    \"winner.spell.count\" as spell_count,\n    \"winner.troop.count\" as troop_count,\n    \"winner.structure.count\" as structure_count,\n    \"winner.trophyChange\" > 0 as won,\n    CASE \n        WHEN \"winner.elixir.average\" > 4.2 THEN 'Beatdown'\n        WHEN \"winner.elixir.average\" < 3.2 THEN 'Cycle'\n        WHEN \"winner.spell.count\" >= 4 THEN 'Spell-Heavy'\n        WHEN \"winner.structure.count\" >= 2 THEN 'Siege'\n        ELSE 'Control'\n    END as archetype\nFROM battles\nWHERE \"winner.elixir.average\" IS NOT NULL\n    AND \"winner.spell.count\" IS NOT NULL\n    AND RANDOM() < 0.1\n\"\"\"\n\nprint(\"Analyzing deck archetypes...\")\narchetype_data = query_to_df(con, archetype_query, show_progress=False)\n\n# Calculate win rates by archetype\narchetype_wr = archetype_data.groupby('archetype').agg({\n    'won': ['sum', 'count']\n}).reset_index()\narchetype_wr.columns = ['archetype', 'wins', 'battles']\narchetype_wr['win_rate'] = archetype_wr['wins'] / archetype_wr['battles']\n\n# Visualization\nfig, axes = plt.subplots(1, 2, figsize=(16, 6))\n\n# Win rates by archetype\nax = axes[0]\ncolors = ['#FF6B6B', '#4ECDC4', '#45B7D1', '#FFA07A', '#98D8C8']\nax.bar(archetype_wr['archetype'], archetype_wr['win_rate'] * 100, \n       color=colors[:len(archetype_wr)], edgecolor='black', alpha=0.8)\nax.set_ylabel('Win Rate (%)', fontsize=14)\nax.set_xlabel('Deck Archetype', fontsize=14)\nax.set_title('Win Rate by Deck Archetype', fontsize=16, fontweight='bold')\nax.axhline(50, color='red', linestyle='--', linewidth=1.5, alpha=0.5, label='50% baseline')\nax.legend()\nax.grid(axis='y', alpha=0.3)\n\n# Usage distribution\nax = axes[1]\nax.bar(archetype_wr['archetype'], archetype_wr['battles'],\n       color=colors[:len(archetype_wr)], edgecolor='black', alpha=0.8)\nax.set_ylabel('Number of Battles', fontsize=14)\nax.set_xlabel('Deck Archetype', fontsize=14)\nax.set_title('Deck Archetype Usage Distribution', fontsize=16, fontweight='bold')\nax.grid(axis='y', alpha=0.3)\n\nplt.tight_layout()\nplt.savefig(os.path.join(PROJECT_ROOT, 'presentation/figures/fig13_deck_archetypes.png'),\n            dpi=300, bbox_inches='tight')\nplt.show()\n\nprint(\"\\nâœ“ Deck Archetype Analysis:\")\nfor idx, row in archetype_wr.iterrows():\n    print(f\"  {row['archetype']}: {row['win_rate']*100:.2f}% WR ({row['battles']:,} battles)\")\n\n# Simple matchup matrix (would need more complex query for full implementation)\nprint(\"\\nâœ“ Archetype analysis complete. Matchup matrix would require pairing winner/loser archetypes.\")",
   "metadata": {},
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "source": "## Section 7: Game Mode Analysis\n\nAnalyze if optimal strategies vary by game mode",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "source": "# Analyze by game mode\ngamemode_query = \"\"\"\nSELECT \n    \"gameMode.id\" as game_mode,\n    AVG(\"winner.elixir.average\") as avg_elixir,\n    AVG(\"winner.spell.count\") as avg_spells,\n    COUNT(*) as battles,\n    SUM(CASE WHEN \"winner.trophyChange\" > 0 THEN 1 ELSE 0 END) as wins\nFROM battles\nWHERE \"gameMode.id\" IS NOT NULL\nGROUP BY \"gameMode.id\"\nHAVING battles > 10000\nORDER BY battles DESC\n\"\"\"\n\nprint(\"Analyzing game modes...\")\ngamemode_data = query_to_df(con, gamemode_query, show_progress=False)\ngamemode_data['win_rate'] = gamemode_data['wins'] / gamemode_data['battles']\n\nprint(\"\\nâœ“ Game Mode Analysis:\")\nprint(f\"  Found {len(gamemode_data)} game modes with >10k battles\")\nprint(\"\\nTop 5 Game Modes:\")\nfor idx, row in gamemode_data.head(5).iterrows():\n    print(f\"  Mode {row['game_mode']}: {row['battles']:,} battles, avg elixir: {row['avg_elixir']:.2f}\")\n\n# Visualization\nif len(gamemode_data) > 1:\n    fig, ax = plt.subplots(figsize=(14, 6))\n    ax.bar(gamemode_data['game_mode'].astype(str), gamemode_data['battles'],\n           color='steelblue', edgecolor='black', alpha=0.7)\n    ax.set_xlabel('Game Mode ID', fontsize=14)\n    ax.set_ylabel('Number of Battles', fontsize=14)\n    ax.set_title('Battle Distribution by Game Mode', fontsize=16, fontweight='bold')\n    ax.grid(axis='y', alpha=0.3)\n    plt.xticks(rotation=45)\n    plt.tight_layout()\n    plt.savefig(os.path.join(PROJECT_ROOT, 'presentation/figures/fig14_game_modes.png'),\n                dpi=300, bbox_inches='tight')\n    plt.show()\nelse:\n    print(\"  Only one game mode found, skipping visualization\")\n\nprint(\"\\nâœ“ Game mode analysis complete\")",
   "metadata": {},
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "source": "## Section 8: Model Insights (Feature Importance)\n\nExtract insights from machine learning models (requires Notebook 06 to be run first)",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "source": "# Placeholder for model insights - requires model training from Notebook 06\nprint(\"âœ“ Model Insights Section\")\nprint(\"  This section requires trained models from Notebook 06\")\nprint(\"  Would include:\")\nprint(\"    - Feature importance rankings from Random Forest\")\nprint(\"    - Top predictive features for winning\")\nprint(\"    - Model calibration analysis\")\nprint(\"    - SHAP values for model interpretability (if applicable)\")\nprint(\"\\n  To implement: Run Notebook 06 first, then load model artifacts here\")\n\n# Placeholder visualization structure\n# if os.path.exists(os.path.join(PROJECT_ROOT, 'artifacts/rf_model_importance.parquet')):\n#     importance_data = pd.read_parquet(os.path.join(PROJECT_ROOT, 'artifacts/rf_model_importance.parquet'))\n#     # Visualize top 15 features\n#     ...\n# else:\n#     print(\"  âš  Model artifacts not found. Run Notebook 06 first.\")",
   "metadata": {},
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "source": "## Section 9: Temporal Meta Evolution\n\nAnalyze if the meta shifts over time (requires battleTime analysis)",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "source": "# Analyze temporal patterns in battle data\ntemporal_query = \"\"\"\nSELECT \n    DATE_TRUNC('week', \"battleTime\"::TIMESTAMP) as week,\n    AVG(\"winner.elixir.average\") as avg_elixir,\n    COUNT(*) as battles\nFROM battles\nWHERE \"battleTime\" IS NOT NULL\n    AND \"battleTime\" != ''\nGROUP BY week\nORDER BY week\nLIMIT 100\n\"\"\"\n\nprint(\"Analyzing temporal meta evolution...\")\ntry:\n    temporal_data = query_to_df(con, temporal_query, show_progress=False)\n    \n    if len(temporal_data) > 5:\n        print(f\"âœ“ Found {len(temporal_data)} weeks of battle data\")\n        print(f\"  Date range: {temporal_data['week'].min()} to {temporal_data['week'].max()}\")\n        \n        # Visualization\n        fig, axes = plt.subplots(2, 1, figsize=(14, 10))\n        \n        # Average elixir over time\n        ax = axes[0]\n        ax.plot(temporal_data['week'], temporal_data['avg_elixir'], \n                marker='o', linewidth=2, color='steelblue')\n        ax.set_xlabel('Week', fontsize=12)\n        ax.set_ylabel('Average Elixir Cost', fontsize=12)\n        ax.set_title('Meta Evolution: Average Elixir Cost Over Time', fontsize=14, fontweight='bold')\n        ax.grid(alpha=0.3)\n        \n        # Battle volume over time\n        ax = axes[1]\n        ax.bar(range(len(temporal_data)), temporal_data['battles'], \n               color='coral', edgecolor='black', alpha=0.7)\n        ax.set_xlabel('Week Index', fontsize=12)\n        ax.set_ylabel('Number of Battles', fontsize=12)\n        ax.set_title('Battle Volume Over Time', fontsize=14, fontweight='bold')\n        ax.grid(axis='y', alpha=0.3)\n        \n        plt.tight_layout()\n        plt.savefig(os.path.join(PROJECT_ROOT, 'presentation/figures/fig15_temporal_meta.png'),\n                    dpi=300, bbox_inches='tight')\n        plt.show()\n        \n        print(\"âœ“ Temporal meta evolution analysis complete\")\n    else:\n        print(\"  âš  Insufficient temporal data (all battles from similar timeframe)\")\n        \nexcept Exception as e:\n    print(f\"  âš  Temporal analysis skipped (battleTime format issue or insufficient data)\")\n    print(f\"  Error: {str(e)}\")",
   "metadata": {},
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "source": "## Section 10: Advanced Insights\n\nDeep dive into underdog wins, close games, and elixir economics",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "source": "# Advanced analysis: Underdog wins, close games, evolution cards\nprint(\"âœ“ Advanced Insights Analysis\")\n\n# 1. Underdog win analysis\nunderdog_query = \"\"\"\nSELECT \n    CASE \n        WHEN \"winner.startingTrophies\" < \"loser.startingTrophies\" - 200 THEN 'Major Underdog (200+ trophy diff)'\n        WHEN \"winner.startingTrophies\" < \"loser.startingTrophies\" THEN 'Minor Underdog'\n        WHEN \"winner.startingTrophies\" > \"loser.startingTrophies\" THEN 'Favorite'\n        ELSE 'Even Match'\n    END as matchup_type,\n    COUNT(*) as battles,\n    AVG(\"winner.elixir.average\") as avg_elixir\nFROM battles\nWHERE \"winner.startingTrophies\" IS NOT NULL \n    AND \"loser.startingTrophies\" IS NOT NULL\nGROUP BY matchup_type\n\"\"\"\n\nunderdog_data = query_to_df(con, underdog_query, show_progress=False)\nprint(\"\\n1. Underdog Analysis:\")\nfor idx, row in underdog_data.iterrows():\n    pct = (row['battles'] / underdog_data['battles'].sum()) * 100\n    print(f\"  {row['matchup_type']}: {row['battles']:,} battles ({pct:.1f}%), avg elixir: {row['avg_elixir']:.2f}\")\n\n# 2. Close game analysis\nclose_game_query = \"\"\"\nSELECT \n    \"winner.crowns\" as winner_crowns,\n    \"loser.crowns\" as loser_crowns,\n    COUNT(*) as battles,\n    AVG(\"winner.elixir.average\") as avg_elixir\nFROM battles\nWHERE \"winner.crowns\" IS NOT NULL \n    AND \"loser.crowns\" IS NOT NULL\nGROUP BY winner_crowns, loser_crowns\nORDER BY winner_crowns, loser_crowns\n\"\"\"\n\nclose_game_data = query_to_df(con, close_game_query, show_progress=False)\nthree_crown = close_game_data[(close_game_data['winner_crowns'] == 3)]['battles'].sum()\none_crown = close_game_data[(close_game_data['winner_crowns'] == 1)]['battles'].sum()\ntotal = close_game_data['battles'].sum()\n\nprint(f\"\\n2. Crown Distribution:\")\nprint(f\"  3-Crown wins: {three_crown:,} ({three_crown/total*100:.1f}%)\")\nprint(f\"  1-Crown wins (close games): {one_crown:,} ({one_crown/total*100:.1f}%)\")\n\n# 3. Evolution card analysis (if any evolution cards detected)\nif cards_loaded:\n    evo_cards = [card_id for card_id, name in card_mapping.items() if is_evolution_card(name)]\n    if len(evo_cards) > 0:\n        print(f\"\\n3. Evolution Cards Found: {len(evo_cards)}\")\n        evo_rankings = card_rankings[card_rankings['is_evolution']]\n        if len(evo_rankings) > 0:\n            print(f\"  Top Evolution Cards:\")\n            for idx, row in evo_rankings.head(5).iterrows():\n                print(f\"    {row['card_name']}: {row['win_rate']*100:.2f}% WR\")\n        else:\n            print(\"  No evolution cards with sufficient usage\")\n    else:\n        print(\"\\n3. No evolution cards detected in dataset\")\nelse:\n    print(\"\\n3. Evolution card analysis skipped (cards.json not loaded)\")\n\n# 4. Elixir economics - win rate per elixir spent\nif len(elixir_data) > 0:\n    elixir_data['wr_per_elixir'] = elixir_data['win_rate'] / elixir_data['elixir_bucket']\n    best_value = elixir_data.loc[elixir_data['wr_per_elixir'].idxmax()]\n    print(f\"\\n4. Elixir Economics (Value Analysis):\")\n    print(f\"  Best value elixir cost: {best_value['elixir_bucket']:.2f}\")\n    print(f\"  Win rate per elixir: {best_value['wr_per_elixir']:.4f}\")\n    print(f\"  Interpretation: Lower cost decks provide better 'bang for buck'\")\n\nprint(\"\\nâœ“ Advanced insights analysis complete\")",
   "metadata": {},
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "source": "## Section 11: Final Recommendations Summary\n\nConsolidate all findings into actionable recommendations",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "source": "# Final recommendations consolidation\nprint(\"=\"*80)\nprint(\"FINAL RECOMMENDATIONS: CLASH ROYALE META ANALYSIS\")\nprint(\"=\"*80)\n\n# Extract key findings from all sections\nrecommendations = {\n    \"Optimal Deck Building\": [],\n    \"Card Selection\": [],\n    \"Trophy-Specific Strategy\": [],\n    \"Avoid These Mistakes\": []\n}\n\n# From Section 2: Deck Characteristics\nif len(elixir_data) > 0:\n    optimal_elixir_val = elixir_data.loc[elixir_data['win_rate'].idxmax(), 'elixir_bucket']\n    recommendations[\"Optimal Deck Building\"].append(\n        f\"Target {optimal_elixir_val:.1f} average elixir cost for best win rate\"\n    )\n\nif len(legendary_analysis) > 0:\n    optimal_leg = legendary_analysis.loc[legendary_analysis['win_rate'].idxmax(), 'legendary_count']\n    recommendations[\"Optimal Deck Building\"].append(\n        f\"Include {optimal_leg:.0f} legendary cards in your deck\"\n    )\n\nif len(top_compositions) > 0:\n    best_comp = top_compositions.iloc[0]\n    recommendations[\"Optimal Deck Building\"].append(\n        f\"Best composition: {best_comp['troop_count']:.0f} troops, {best_comp['spell_count']:.0f} spells, {best_comp['structure_count']:.0f} structures\"\n    )\n\n# From Section 4: Card Rankings\nif len(card_rankings) > 0:\n    top_3_cards = card_rankings.head(3)['card_name'].tolist()\n    recommendations[\"Card Selection\"].append(\n        f\"Top 3 meta cards: {', '.join(top_3_cards)}\"\n    )\n\n# From Section 3: Synergies\nif len(top_synergies) > 0:\n    best_synergy = top_synergies.iloc[0]\n    recommendations[\"Card Selection\"].append(\n        f\"Best synergy: {best_synergy['card1_name']} + {best_synergy['card2_name']} (Lift: {best_synergy['lift']:.2f})\"\n    )\n\nif len(top_anti) > 0:\n    worst_synergy = top_anti.iloc[0]\n    recommendations[\"Avoid These Mistakes\"].append(\n        f\"Avoid pairing: {worst_synergy['card1_name']} + {worst_synergy['card2_name']} (Lift: {worst_synergy['lift']:.2f})\"\n    )\n\n# From Section 5: Trophy-Specific\nif len(trophy_brackets) > 0:\n    recommendations[\"Trophy-Specific Strategy\"].append(\n        \"Meta varies significantly across trophy brackets - check heatmap for your level\"\n    )\n\n# From Section 6: Archetypes\nif len(archetype_wr) > 0:\n    best_archetype = archetype_wr.loc[archetype_wr['win_rate'].idxmax()]\n    recommendations[\"Optimal Deck Building\"].append(\n        f\"Best performing archetype: {best_archetype['archetype']} ({best_archetype['win_rate']*100:.1f}% WR)\"\n    )\n\n# Print recommendations\nfor category, items in recommendations.items():\n    if len(items) > 0:\n        print(f\"\\n{category}:\")\n        for item in items:\n            print(f\"  â€¢ {item}\")\n\n# Summary statistics\nprint(\"\\n\" + \"=\"*80)\nprint(\"ANALYSIS SUMMARY\")\nprint(\"=\"*80)\nprint(f\"Cards analyzed: {len(card_rankings) if len(card_rankings) > 0 else 'N/A'}\")\nprint(f\"Card pairs analyzed: {len(pair_data) if len(pair_data) > 0 else 'N/A'}\")\nprint(f\"Trophy brackets: {len(trophy_brackets)}\")\nprint(f\"Deck archetypes identified: {len(archetype_wr) if len(archetype_wr) > 0 else 'N/A'}\")\nprint(f\"\\nAll visualizations saved to: presentation/figures/\")\nprint(f\"All data artifacts saved to: artifacts/\")\nprint(\"=\"*80)\n\n# Save final recommendations to file\nrecommendations_df = pd.DataFrame([\n    {\"category\": cat, \"recommendation\": rec}\n    for cat, recs in recommendations.items()\n    for rec in recs\n])\nrecommendations_df.to_csv(os.path.join(PROJECT_ROOT, 'artifacts/final_recommendations.csv'), index=False)\nprint(\"\\nâœ“ Final recommendations saved to artifacts/final_recommendations.csv\")\n\nprint(\"\\n\" + \"=\"*80)\nprint(\"NOTEBOOK 04.5 COMPLETE - ADVANCED META ANALYSIS\")\nprint(\"=\"*80)",
   "metadata": {},
   "execution_count": null,
   "outputs": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.8.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}