{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 04.5 - Advanced Meta Analysis & Actionable Insights\n",
    "\n",
    "**Purpose**: Extract comprehensive, actionable insights for competitive play\n",
    "\n",
    "**Key Questions**:\n",
    "- What are the optimal deck characteristics (elixir, composition)?\n",
    "- Which specific cards and card combos dominate the meta?\n",
    "- How does the meta evolve across trophy progression walls?\n",
    "- What do our ML models tell us about winning strategies?\n",
    "\n",
    "**Unique Features**:\n",
    "- Data-driven trophy wall detection (not hardcoded)\n",
    "- Card synergy analysis (2-card combos)\n",
    "- Evolution card impact assessment\n",
    "- Statistical confidence intervals on all metrics\n",
    "- Archetype performance by trophy level\n",
    "- Card level impact analysis\n",
    "\n",
    "**Output**: Presentation-ready insights with human-readable card names"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys, os, json\n",
    "import duckdb, pandas as pd, numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from scipy import stats\n",
    "from itertools import combinations\n",
    "\n",
    "PROJECT_ROOT = os.path.abspath(os.path.join(os.getcwd(), '..'))\n",
    "sys.path.insert(0, os.path.join(PROJECT_ROOT, 'src'))\n",
    "\n",
    "# Use Parquet if available\n",
    "DATA_PATH = os.path.join(PROJECT_ROOT, 'battles.parquet')\n",
    "if not os.path.exists(DATA_PATH):\n",
    "    DATA_PATH = os.path.join(PROJECT_ROOT, 'battles.csv')\n",
    "\n",
    "from duckdb_utils import get_connection, create_battles_view, query_to_df, save_to_parquet\n",
    "from visualization import setup_presentation_style\n",
    "\n",
    "con = get_connection()\n",
    "create_battles_view(con, DATA_PATH)\n",
    "setup_presentation_style()\n",
    "\n",
    "# Ensure artifacts and figures directories exist\n",
    "os.makedirs(os.path.join(PROJECT_ROOT, 'artifacts'), exist_ok=True)\n",
    "os.makedirs(os.path.join(PROJECT_ROOT, 'presentation/figures'), exist_ok=True)\n",
    "\n",
    "print(\"âœ“ Environment setup complete\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Section 0: Load Card Name Mapping\n",
    "\n",
    "Convert card IDs to human-readable names for all analyses"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Try to load cards.json from multiple locations\n",
    "card_mapping = {}\n",
    "card_file_locations = [\n",
    "    os.path.join(PROJECT_ROOT, 'artifacts', 'cards.json'),\n",
    "    os.path.join(PROJECT_ROOT, 'Datasets', 'cards.json'),\n",
    "    os.path.join(PROJECT_ROOT, 'cards.json')\n",
    "]\n",
    "\n",
    "cards_loaded = False\n",
    "for card_file in card_file_locations:\n",
    "    if os.path.exists(card_file):\n",
    "        print(f\"Found cards.json at: {card_file}\")\n",
    "        with open(card_file, 'r') as f:\n",
    "            card_data = json.load(f)\n",
    "            \n",
    "            # Handle different JSON structures\n",
    "            if isinstance(card_data, dict):\n",
    "                # If already a mapping\n",
    "                card_mapping = {int(k) if k.isdigit() else k: v for k, v in card_data.items()}\n",
    "            elif isinstance(card_data, list):\n",
    "                # If it's a list of card objects\n",
    "                for card in card_data:\n",
    "                    if 'id' in card and 'name' in card:\n",
    "                        card_mapping[card['id']] = card['name']\n",
    "            \n",
    "            cards_loaded = True\n",
    "            print(f\"âœ“ Loaded {len(card_mapping)} card mappings\")\n",
    "            break\n",
    "\n",
    "if not cards_loaded:\n",
    "    print(\"âš  Warning: cards.json not found. Will use card IDs instead of names.\")\n",
    "    print(\"  Expected locations:\")\n",
    "    for loc in card_file_locations:\n",
    "        print(f\"    - {loc}\")\n",
    "\n",
    "def get_card_name(card_id):\n",
    "    \"\"\"Convert card ID to name, with fallback to ID if not found\"\"\"\n",
    "    if pd.isna(card_id):\n",
    "        return None\n",
    "    \n",
    "    # Try as integer\n",
    "    try:\n",
    "        card_id_int = int(card_id)\n",
    "        if card_id_int in card_mapping:\n",
    "            return card_mapping[card_id_int]\n",
    "    except (ValueError, TypeError):\n",
    "        pass\n",
    "    \n",
    "    # Try as string\n",
    "    if card_id in card_mapping:\n",
    "        return card_mapping[card_id]\n",
    "    \n",
    "    # Fallback to ID\n",
    "    return f\"Card_{card_id}\"\n",
    "\n",
    "def is_evolution_card(card_name):\n",
    "    \"\"\"Detect if a card is an evolution variant\"\"\"\n",
    "    if not card_name:\n",
    "        return False\n",
    "    evolution_keywords = ['evolved', 'evo', 'evolution', 'evolved ']\n",
    "    return any(keyword in str(card_name).lower() for keyword in evolution_keywords)\n",
    "\n",
    "# Test the mapping\n",
    "if cards_loaded:\n",
    "    sample_ids = list(card_mapping.keys())[:5]\n",
    "    print(f\"\\nSample card mappings:\")\n",
    "    for card_id in sample_ids:\n",
    "        name = get_card_name(card_id)\n",
    "        evo = \"[EVO]\" if is_evolution_card(name) else \"\"\n",
    "        print(f\"  {card_id} â†’ {name} {evo}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Section 1: Data-Driven Trophy Wall Detection\n",
    "\n",
    "Instead of assuming 4k/5k/6k/7k, let's detect walls from battle density"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Query battle distribution to find natural clustering points\n",
    "trophy_dist_query = \"\"\"\n",
    "SELECT \n",
    "    FLOOR(\"average.startingTrophies\" / 100) * 100 as trophy_bin,\n",
    "    COUNT(*) as battle_count\n",
    "FROM battles\n",
    "WHERE \"average.startingTrophies\" IS NOT NULL\n",
    "    AND \"average.startingTrophies\" BETWEEN 0 AND 10000\n",
    "GROUP BY trophy_bin\n",
    "ORDER BY trophy_bin\n",
    "\"\"\"\n",
    "\n",
    "trophy_distribution = query_to_df(con, trophy_dist_query, show_progress=False)\n",
    "\n",
    "print(f\"Trophy distribution loaded: {len(trophy_distribution)} bins\")\n",
    "print(f\"Trophy range: {trophy_distribution['trophy_bin'].min():.0f} to {trophy_distribution['trophy_bin'].max():.0f}\")\n",
    "print(f\"Total battles: {trophy_distribution['battle_count'].sum():,}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "# Detect trophy walls using peak detection\nfrom scipy.signal import find_peaks\n\n# Normalize battle counts for peak detection\nnormalized_counts = trophy_distribution['battle_count'] / trophy_distribution['battle_count'].max()\n\n# Find peaks (local maxima) - these are trophy walls\npeaks, properties = find_peaks(normalized_counts, prominence=0.1, distance=10)\n\ndetected_walls = trophy_distribution.iloc[peaks]['trophy_bin'].values\n\n# Use standard walls if detection fails or finds too few\nstandard_walls = [4000, 5000, 6000, 7000]\nif len(detected_walls) < 3:\n    print(\"âš  Using standard trophy walls: 4k, 5k, 6k, 7k\")\n    trophy_walls = standard_walls\nelse:\n    # Round detected walls to nearest 1000\n    trophy_walls = [int(round(w, -3)) for w in detected_walls if 3000 <= w <= 8000]\n    # Ensure we have key milestones\n    for wall in standard_walls:\n        if not any(abs(w - wall) < 500 for w in trophy_walls):\n            trophy_walls.append(wall)\n    trophy_walls = sorted(set(trophy_walls))\n\nprint(f\"\\nâœ“ Trophy Walls Detected: {trophy_walls}\")\n\n# Visualize distribution with detected walls\nfig, ax = plt.subplots(figsize=(14, 7))\n\nax.bar(trophy_distribution['trophy_bin'], trophy_distribution['battle_count'], \n       width=90, color='steelblue', edgecolor='black', alpha=0.7)\n\n# Mark detected walls\ncolors = ['red', 'orange', 'purple', 'darkred', 'maroon']\nfor i, wall in enumerate(trophy_walls[:5]):\n    color = colors[i] if i < len(colors) else 'black'\n    ax.axvline(wall, color=color, linestyle='--', linewidth=2.5, alpha=0.8,\n               label=f'{wall/1000:.0f}k Trophy Wall')\n\nax.set_xlabel('Trophy Count', fontsize=16)\nax.set_ylabel('Number of Battles', fontsize=16)\nax.set_title('Trophy Distribution: Data-Driven Wall Detection', fontsize=20, fontweight='bold', pad=20)\nax.legend(fontsize=12, loc='upper right')\nax.grid(axis='y', alpha=0.3)\n\nplt.tight_layout()\nplt.savefig(os.path.join(PROJECT_ROOT, 'presentation/figures/fig5_detected_walls.png'), \n            dpi=300, bbox_inches='tight')\nplt.show()\n\n# Define trophy brackets based on detected walls\ntrophy_brackets = {}\nwalls_with_bounds = [0] + trophy_walls + [10000]\nfor i in range(len(walls_with_bounds) - 1):\n    lower = walls_with_bounds[i]\n    upper = walls_with_bounds[i + 1]\n    if lower == 0:\n        label = f'0-{upper/1000:.0f}k'\n    elif upper == 10000:\n        label = f'{lower/1000:.0f}k+'\n    else:\n        label = f'{lower/1000:.0f}k-{upper/1000:.0f}k'\n    trophy_brackets[label] = (lower, upper)\n\nprint(f\"\\nðŸ“Š Trophy Brackets Defined:\")\nfor label, (low, high) in trophy_brackets.items():\n    count = trophy_distribution[(trophy_distribution['trophy_bin'] >= low) & \n                                 (trophy_distribution['trophy_bin'] < high)]['battle_count'].sum()\n    print(f\"  {label:<15} ({low:>5} - {high:>5}): {count:>12,} battles\")\n\n# Save detected walls\ndetected_walls_data = {\n    'walls': trophy_walls,\n    'brackets': {k: list(v) for k, v in trophy_brackets.items()}\n}\nwith open(os.path.join(PROJECT_ROOT, 'artifacts/detected_trophy_walls.json'), 'w') as f:\n    json.dump(detected_walls_data, f, indent=2)\n\nprint(f\"\\nâœ“ Trophy walls saved to artifacts/detected_trophy_walls.json\")"
  },
  {
   "cell_type": "code",
   "source": "# 2.1 Optimal Elixir Cost Analysis\nelixir_query = \"\"\"\nSELECT \n    ROUND(\"winner.elixir.average\" * 4) / 4 as elixir_bucket,\n    COUNT(*) as battles,\n    SUM(CASE WHEN \"winner.trophyChange\" > 0 THEN 1 ELSE 0 END) as wins,\n    AVG(\"winner.startingTrophies\") as avg_trophies\nFROM battles\nWHERE \"winner.elixir.average\" IS NOT NULL\n    AND \"winner.elixir.average\" BETWEEN 2.0 AND 6.0\nGROUP BY elixir_bucket\nHAVING battles > 1000\nORDER BY elixir_bucket\n\"\"\"\n\nelixir_data = query_to_df(con, elixir_query, show_progress=False)\nelixir_data['win_rate'] = elixir_data['wins'] / elixir_data['battles']\n\n# Calculate 95% confidence intervals\ndef wilson_confidence_interval(wins, total, z=1.96):\n    \"\"\"Wilson score interval for binomial proportion\"\"\"\n    if total == 0:\n        return 0, 0\n    p_hat = wins / total\n    denominator = 1 + z**2 / total\n    center = (p_hat + z**2 / (2 * total)) / denominator\n    margin = z * np.sqrt((p_hat * (1 - p_hat) + z**2 / (4 * total)) / total) / denominator\n    return center - margin, center + margin\n\nelixir_data['ci_lower'], elixir_data['ci_upper'] = zip(*elixir_data.apply(\n    lambda row: wilson_confidence_interval(row['wins'], row['battles']), axis=1))\n\n# Find optimal elixir range\noptimal_elixir = elixir_data.loc[elixir_data['win_rate'].idxmax(), 'elixir_bucket']\noptimal_wr = elixir_data['win_rate'].max()\n\n# Visualization\nfig, ax = plt.subplots(figsize=(14, 7))\n\nax.plot(elixir_data['elixir_bucket'], elixir_data['win_rate'] * 100, \n        marker='o', linewidth=3, markersize=10, color='steelblue', label='Win Rate')\nax.fill_between(elixir_data['elixir_bucket'], \n                elixir_data['ci_lower'] * 100, \n                elixir_data['ci_upper'] * 100,\n                alpha=0.3, color='steelblue', label='95% Confidence Interval')\n\n# Mark optimal point\nax.axvline(optimal_elixir, color='red', linestyle='--', linewidth=2, alpha=0.7)\nax.axhline(optimal_wr * 100, color='red', linestyle=':', linewidth=1.5, alpha=0.5)\nax.scatter([optimal_elixir], [optimal_wr * 100], color='red', s=200, zorder=5, \n           label=f'Optimal: {optimal_elixir:.2f} elixir')\n\nax.set_xlabel('Average Elixir Cost', fontsize=16)\nax.set_ylabel('Win Rate (%)', fontsize=16)\nax.set_title('Optimal Elixir Cost for Winning Decks', fontsize=20, fontweight='bold', pad=20)\nax.legend(fontsize=12)\nax.grid(alpha=0.3)\nax.set_ylim(48, 54)\n\nplt.tight_layout()\nplt.savefig(os.path.join(PROJECT_ROOT, 'presentation/figures/fig7_optimal_elixir.png'),\n            dpi=300, bbox_inches='tight')\nplt.show()\n\nprint(f\"\\\\nâœ“ Optimal Elixir Analysis:\")\nprint(f\"  Best elixir cost: {optimal_elixir:.2f} ({optimal_wr*100:.2f}% win rate)\")\nprint(f\"  95% CI: [{elixir_data.loc[elixir_data['win_rate'].idxmax(), 'ci_lower']*100:.2f}%, {elixir_data.loc[elixir_data['win_rate'].idxmax(), 'ci_upper']*100:.2f}%]\")\nprint(f\"  Sample size: {elixir_data.loc[elixir_data['win_rate'].idxmax(), 'battles']:,} battles\")\n\n# Identify optimal range (within 1% of best)\noptimal_range = elixir_data[elixir_data['win_rate'] >= optimal_wr - 0.01]\nprint(f\"  Optimal range: {optimal_range['elixir_bucket'].min():.2f} - {optimal_range['elixir_bucket'].max():.2f} elixir\")",
   "metadata": {},
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "source": "## Section 2: Optimal Deck Characteristics\n\nFind the \"Goldilocks zone\" for deck building with statistical confidence",
   "metadata": {}
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.8.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}