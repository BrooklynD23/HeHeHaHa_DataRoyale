# Competition Judging Rubric

## Overview

**Format**: 8-minute presentation + 2-4 minutes Q&A
**Deadline**: Friday the 14th, 11:30 AM (email to ajsantos@cpp.edu)
**Judging**: Friday the 14th, 12:15 PM start time
**Results**: Friday the 14th, 3:30 PM

## Evaluation Criteria

Judges will assess each presentation across **five key categories**:

---

### 1. Clarity and Storytelling (20%)

**What judges look for**:
- ✅ Coherent narrative from problem → insight → impact
- ✅ Well-structured and understandable for a non-technical audience
- ✅ Concise, clear, engaging points
- ✅ Logical flow between sections

**How to excel**:
- Start with a clear problem statement: "We're investigating X because Y"
- Use transitions: "Now that we've seen WHAT happens, let's explore WHY..."
- End each insight with impact: "This means players should..."
- Avoid jargon; explain technical terms when necessary

**Common pitfalls**:
- ❌ Jumping between unrelated findings
- ❌ No clear "So what?" for insights
- ❌ Too technical for general audience
- ❌ Rushed conclusion without synthesis

**Scoring guide**:
- **Excellent (18-20)**: Compelling story, seamless flow, accessible to all
- **Good (15-17)**: Clear structure, mostly accessible, minor gaps
- **Satisfactory (12-14)**: Understandable but disjointed or overly technical
- **Needs Work (0-11)**: Confusing narrative, heavy jargon, unclear purpose

---

### 2. Data Understanding and Exploration (20%)

**What judges look for**:
- ✅ Meaningful insights beyond surface-level observations
- ✅ Depth of analysis (not just descriptive statistics)
- ✅ Acknowledgment of data limitations or biases
- ✅ Evidence of thorough exploration

**How to excel**:
- Go beyond "Card X has 55% win rate" → "Card X wins 55% BUT only when paired with Y"
- Compare across dimensions: "At low trophies, deck A dominates. At high trophies, deck B takes over"
- Acknowledge gaps: "We found Z pattern, though our data lacks player skill ratings which could explain..."
- Show you explored multiple angles before settling on key insights

**Common pitfalls**:
- ❌ Only showing basic counts/averages
- ❌ Cherry-picking data without context
- ❌ Ignoring confounding factors
- ❌ Not addressing obvious data quality issues

**Scoring guide**:
- **Excellent (18-20)**: Deep insights, nuanced understanding, limitations noted
- **Good (15-17)**: Solid analysis, some depth, aware of limitations
- **Satisfactory (12-14)**: Surface-level findings, minimal depth
- **Needs Work (0-11)**: Only basic statistics, no real insights

---

### 3. Modeling and Technical Rigor (20%)

**What judges look for**:
- ✅ Appropriate analytical or modeling techniques for the problem
- ✅ Proper validation and evaluation (metrics, train/test splits)
- ✅ Demonstrated real-world application or practical relevance

**How to excel**:
- Justify method choice: "We used Random Forest because feature importance helps us understand what matters"
- Show validation: "We used 80/20 train-test split with cross-validation, achieving X% accuracy"
- Compare approaches: "Logistic regression gave 52%, but XGBoost achieved 58%"
- Connect to benchmarks: "Previous research achieved 56.94%; we improved to 58%"

**Common pitfalls**:
- ❌ No train/test split (overfitting)
- ❌ Using complex models without explanation
- ❌ No evaluation metrics shown
- ❌ Method doesn't match problem (e.g., regression for classification)

**Scoring guide**:
- **Excellent (18-20)**: Rigorous methods, proper validation, justified choices
- **Good (15-17)**: Sound approach, basic validation, reasonable choices
- **Satisfactory (12-14)**: Correct methods but weak validation
- **Needs Work (0-11)**: Poor method choice or no validation

---

### 4. Insights and Recommendations (20%)

**What judges look for**:
- ✅ Actionable conclusions supported by data
- ✅ Connection to real-world decisions, business strategies, or social impact
- ✅ Clear recommendations for different stakeholders

**How to excel**:
- Make specific recommendations: "Players should use 3.5-4.0 avg elixir decks for consistent wins"
- Target multiple audiences:
  - For players: "Try deck archetype X at trophy level Y"
  - For game designers: "Evolution cards may need balance adjustments"
  - For esports: "Meta shifts favor low-elixir cycle decks"
- Quantify impact: "Switching to optimal elixir range could improve win rate by 8%"

**Common pitfalls**:
- ❌ Vague recommendations: "Players should choose good cards"
- ❌ Insights without action: "We found pattern X" (but so what?)
- ❌ Recommendations not backed by data
- ❌ No business/real-world connection

**Scoring guide**:
- **Excellent (18-20)**: Highly actionable, data-backed, multi-stakeholder impact
- **Good (15-17)**: Clear recommendations, data support, some specificity
- **Satisfactory (12-14)**: Generic recommendations, weak data link
- **Needs Work (0-11)**: No actionable insights or unsupported claims

---

### 5. Visuals and Delivery (20%)

**What judges look for**:
- ✅ Clear, well-labeled charts and graphs
- ✅ Visuals are easy to interpret
- ✅ Readable fonts, good color contrast
- ✅ Confident, engaging, professional delivery

**How to excel**:

**Visuals**:
- Use chart titles that state the insight: "Evolution Cards Dominate the Meta" (not "Card Win Rates")
- Label axes with units: "Win Rate (%)" not just "Rate"
- Use colorblind-friendly palettes
- Annotate key points: Arrow pointing to outlier with explanation
- Font size 18+ (readable from back of room)

**Delivery**:
- Make eye contact, smile
- Speak clearly and with energy
- Point to charts as you explain them
- Stay within time (8 min = strict cutoff)
- Handle Q&A gracefully: "Great question. Based on our data, we found X..."

**Common pitfalls**:
- ❌ Tiny fonts (can't read from audience)
- ❌ Cluttered charts with too much info
- ❌ Poor color choices (red/green for colorblind)
- ❌ Reading directly from slides
- ❌ Going over time limit

**Scoring guide**:
- **Excellent (18-20)**: Professional visuals, engaging delivery, perfect timing
- **Good (15-17)**: Clear visuals, confident delivery, good timing
- **Satisfactory (12-14)**: Acceptable visuals, adequate delivery, minor issues
- **Needs Work (0-11)**: Poor visuals or delivery, major timing issues

---

## Scoring Summary

| Category | Weight | Excellent | Good | Satisfactory | Needs Work |
|----------|--------|-----------|------|--------------|------------|
| Clarity & Storytelling | 20% | 18-20 | 15-17 | 12-14 | 0-11 |
| Data Understanding | 20% | 18-20 | 15-17 | 12-14 | 0-11 |
| Technical Rigor | 20% | 18-20 | 15-17 | 12-14 | 0-11 |
| Insights & Recommendations | 20% | 18-20 | 15-17 | 12-14 | 0-11 |
| Visuals & Delivery | 20% | 18-20 | 15-17 | 12-14 | 0-11 |
| **TOTAL** | **100%** | **90-100** | **75-89** | **60-74** | **0-59** |

---

## Tips for Success

### Before the Presentation

1. **Tell a story**: Guide the audience through your thought process
2. **Keep it simple**: Avoid unnecessary jargon for general audience
3. **Show evidence**: Every claim needs data backup
4. **Highlight limitations**: Acknowledging what you can't do shows maturity
5. **Polish slides**: Visuals support narrative, not distract from it
6. **Practice timing**: Rehearse to ensure 7:30-8:00 delivery

### During the Presentation

1. **Start strong**: Hook the audience in the first 30 seconds
2. **Use signposting**: "First, I'll show X. Then we'll explore Y. Finally, Z."
3. **Pause for emphasis**: Let key insights land
4. **Engage with visuals**: Point, circle, highlight what matters
5. **End with impact**: "So in summary, we found X, Y, Z. This means..."

### During Q&A

1. **Listen fully**: Don't interrupt, understand the question
2. **Acknowledge**: "That's a great question about X"
3. **Answer concisely**: 30-60 seconds max
4. **Use data**: "Based on our analysis, we observed..."
5. **Admit gaps**: "We didn't explore that, but it would be valuable future work"
6. **Stay positive**: Critiques are opportunities to show thoughtfulness

---

## Common Judge Questions

Be prepared to answer:

**Data & Methods**:
- "How did you handle missing data?"
- "Why did you choose this model over alternatives?"
- "How did you validate your findings?"
- "What sampling strategy did you use?"

**Insights & Impact**:
- "How would you deploy this in production?"
- "What business decisions would you recommend?"
- "What surprised you most in the data?"
- "How generalizable are your findings?"

**Limitations & Future Work**:
- "What are the biggest limitations of your analysis?"
- "What would you do differently with more time?"
- "What additional data would improve your model?"
- "How confident are you in these results?"

---

## Judging Panel

Expect a mix of:
- **Faculty members**: Look for statistical rigor, proper methodology
- **Industry professionals**: Want to see business impact, real-world application
- **Data science mentors**: Appreciate technical depth and best practices

**Key insight**: Balance technical rigor (for technical judges) with clear communication (for all judges).

---

## Final Reminders

**Submission**:
- Slide deck due: **Friday 14th, 11:30 AM**
- Email to: **ajsantos@cpp.edu**
- Format: PowerPoint or PDF

**Presentation**:
- Length: **8 minutes** (strict cutoff)
- Q&A: **2-4 minutes**
- Time: **Friday 14th, 12:15 PM start**

**Results**:
- Announced: **Friday 14th, 3:30 PM**

---

## What Judges Remember

Weeks after competition, judges remember:

1. **The story**: Did you tell a compelling narrative?
2. **The insight**: What was the "aha!" moment?
3. **The delivery**: Were you confident and clear?
4. **The visual**: Was there one chart that perfectly captured the point?

They DON'T remember:
- Exact accuracy percentages
- Every statistical test you ran
- Minor details on slides

**Focus on**: Memorable insights + clear communication + professional delivery

**Good luck! Make your story count.**
